01/10/2021 05:29:53 1
01/10/2021 05:29:53 Launching the MT-DNN training
01/10/2021 05:29:53 Loading data/canonical_data/bert_base_uncased_lower/mnli_train.json as task 0
01/10/2021 05:30:02 ####################
01/10/2021 05:30:02 {'log_file': 'checkpoints/finetune-mnli-LM_entropy8-$/log.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'checkpoints/mtdnn-LM_entropy8/model_4.pt', 'data_dir': 'data/canonical_data/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'experiments/glue/glue_task_def.yml', 'train_datasets': ['mnli'], 'test_datasets': ['mnli'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'update_bert_opt': 0, 'multi_gpu_on': True, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 10, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0.0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoints/finetune-mnli-LM_entropy8-$', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 0.001, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'loss_pred': True, 'collect_uncertainty': None, 'collect_topk': 0.1, 'load_ranked_data': None, 'mc_dropout': 0, 'finetune': True, 'encode_mode': False, 'task_def_list': [{'self': '{}', 'label_vocab': '<data_utils.vocab.Vocabulary object at 0x7ff465fbd580>', 'n_class': '3', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.ACC: 0>,)', 'split_names': "['train', 'matched_dev', 'mismatched_dev', 'matched_test', 'mismatched_test']", 'enable_san': 'False', 'dropout_p': '0.1', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': '<LossCriterion.MseCriterion: 1>', 'adv_loss': '<LossCriterion.SymKlCriterion: 7>', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
01/10/2021 05:30:02 ####################
01/10/2021 05:30:02 ############# Gradient Accumulation Info #############
01/10/2021 05:30:02 number of step: 490880
01/10/2021 05:30:02 number of grad grad_accumulation step: 1
01/10/2021 05:30:02 adjusted number of step: 490880
01/10/2021 05:30:02 ############# Gradient Accumulation Info #############
01/10/2021 05:30:12 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
    (1): DropoutWrapper()
    (2): DropoutWrapper()
    (3): DropoutWrapper()
    (4): DropoutWrapper()
    (5): DropoutWrapper()
    (6): DropoutWrapper()
    (7): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (loss_pred_fc): Linear(in_features=768, out_features=1, bias=True)
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=3, bias=True)
    (1): Linear(in_features=768, out_features=2, bias=True)
    (2): Linear(in_features=768, out_features=2, bias=True)
    (3): Linear(in_features=768, out_features=2, bias=True)
    (4): Linear(in_features=768, out_features=2, bias=True)
    (5): Linear(in_features=768, out_features=2, bias=True)
    (6): Linear(in_features=768, out_features=2, bias=True)
    (7): Linear(in_features=768, out_features=1, bias=True)
  )
)

01/10/2021 05:30:12 Total number of params: 109495313
01/10/2021 05:30:12 At epoch 0
01/10/2021 05:30:12 Task [ 0] updates[     1] train loss[1.44471] remaining[4:08:49]
01/10/2021 05:31:21 Task [ 0] updates[   500] train loss[1.33173] remaining[1:51:34]
01/10/2021 05:32:29 Task [ 0] updates[  1000] train loss[1.25381] remaining[1:50:04]
01/10/2021 05:33:37 Task [ 0] updates[  1500] train loss[1.25687] remaining[1:48:21]
01/10/2021 05:34:45 Task [ 0] updates[  2000] train loss[1.23163] remaining[1:47:11]
01/10/2021 05:35:54 Task [ 0] updates[  2500] train loss[1.19350] remaining[1:46:04]
01/10/2021 05:37:02 Task [ 0] updates[  3000] train loss[1.15905] remaining[1:44:50]
01/10/2021 05:38:10 Task [ 0] updates[  3500] train loss[1.10472] remaining[1:43:41]
01/10/2021 05:39:18 Task [ 0] updates[  4000] train loss[1.05680] remaining[1:42:29]
01/10/2021 05:40:26 Task [ 0] updates[  4500] train loss[1.01464] remaining[1:41:22]
01/10/2021 05:41:35 Task [ 0] updates[  5000] train loss[0.97578] remaining[1:40:17]
01/10/2021 05:42:44 Task [ 0] updates[  5500] train loss[0.94587] remaining[1:39:19]
01/10/2021 05:43:53 Task [ 0] updates[  6000] train loss[0.91874] remaining[1:38:16]
01/10/2021 05:45:02 Task [ 0] updates[  6500] train loss[0.89601] remaining[1:37:09]
01/10/2021 05:46:11 Task [ 0] updates[  7000] train loss[0.87514] remaining[1:36:06]
01/10/2021 05:47:19 Task [ 0] updates[  7500] train loss[0.85888] remaining[1:34:55]
01/10/2021 05:48:28 Task [ 0] updates[  8000] train loss[0.84362] remaining[1:33:47]
01/10/2021 05:49:37 Task [ 0] updates[  8500] train loss[0.82898] remaining[1:32:40]
01/10/2021 05:50:45 Task [ 0] updates[  9000] train loss[0.81484] remaining[1:31:32]
01/10/2021 05:51:54 Task [ 0] updates[  9500] train loss[0.80289] remaining[1:30:25]
01/10/2021 05:53:03 Task [ 0] updates[ 10000] train loss[0.79170] remaining[1:29:20]
01/10/2021 05:54:11 Task [ 0] updates[ 10500] train loss[0.78176] remaining[1:28:08]
01/10/2021 05:55:19 Task [ 0] updates[ 11000] train loss[0.77213] remaining[1:26:57]
01/10/2021 05:56:26 Task [ 0] updates[ 11500] train loss[0.76412] remaining[1:25:45]
01/10/2021 05:57:36 Task [ 0] updates[ 12000] train loss[0.75647] remaining[1:24:40]
01/10/2021 05:58:46 Task [ 0] updates[ 12500] train loss[0.74953] remaining[1:23:35]
01/10/2021 05:59:54 Task [ 0] updates[ 13000] train loss[0.74252] remaining[1:22:27]
01/10/2021 06:01:04 Task [ 0] updates[ 13500] train loss[0.73618] remaining[1:21:22]
01/10/2021 06:02:13 Task [ 0] updates[ 14000] train loss[0.73054] remaining[1:20:15]
01/10/2021 06:03:21 Task [ 0] updates[ 14500] train loss[0.72509] remaining[1:19:04]
01/10/2021 06:04:29 Task [ 0] updates[ 15000] train loss[0.71980] remaining[1:17:54]
01/10/2021 06:05:38 Task [ 0] updates[ 15500] train loss[0.71466] remaining[1:16:47]
01/10/2021 06:06:47 Task [ 0] updates[ 16000] train loss[0.71031] remaining[1:15:39]
01/10/2021 06:07:56 Task [ 0] updates[ 16500] train loss[0.70567] remaining[1:14:31]
01/10/2021 06:09:04 Task [ 0] updates[ 17000] train loss[0.70166] remaining[1:13:22]
01/10/2021 06:10:14 Task [ 0] updates[ 17500] train loss[0.69726] remaining[1:12:14]
01/10/2021 06:11:23 Task [ 0] updates[ 18000] train loss[0.69338] remaining[1:11:07]
01/10/2021 06:12:31 Task [ 0] updates[ 18500] train loss[0.68957] remaining[1:09:57]
01/10/2021 06:13:39 Task [ 0] updates[ 19000] train loss[0.68542] remaining[1:08:47]
01/10/2021 06:14:47 Task [ 0] updates[ 19500] train loss[0.68154] remaining[1:07:39]
01/10/2021 06:15:56 Task [ 0] updates[ 20000] train loss[0.67854] remaining[1:06:31]
01/10/2021 06:17:05 Task [ 0] updates[ 20500] train loss[0.67524] remaining[1:05:22]
01/10/2021 06:18:13 Task [ 0] updates[ 21000] train loss[0.67218] remaining[1:04:13]
01/10/2021 06:19:22 Task [ 0] updates[ 21500] train loss[0.66973] remaining[1:03:04]
01/10/2021 06:20:31 Task [ 0] updates[ 22000] train loss[0.66632] remaining[1:01:56]
01/10/2021 06:21:40 Task [ 0] updates[ 22500] train loss[0.66364] remaining[1:00:48]
01/10/2021 06:22:48 Task [ 0] updates[ 23000] train loss[0.66106] remaining[0:59:39]
01/10/2021 06:23:56 Task [ 0] updates[ 23500] train loss[0.65817] remaining[0:58:30]
01/10/2021 06:25:06 Task [ 0] updates[ 24000] train loss[0.65533] remaining[0:57:22]
01/10/2021 06:26:15 Task [ 0] updates[ 24500] train loss[0.65312] remaining[0:56:14]
01/10/2021 06:27:23 Task [ 0] updates[ 25000] train loss[0.65049] remaining[0:55:05]
01/10/2021 06:28:31 Task [ 0] updates[ 25500] train loss[0.64816] remaining[0:53:56]
01/10/2021 06:29:39 Task [ 0] updates[ 26000] train loss[0.64569] remaining[0:52:47]
01/10/2021 06:30:47 Task [ 0] updates[ 26500] train loss[0.64339] remaining[0:51:38]
01/10/2021 06:31:56 Task [ 0] updates[ 27000] train loss[0.64128] remaining[0:50:29]
01/10/2021 06:33:04 Task [ 0] updates[ 27500] train loss[0.63941] remaining[0:49:21]
01/10/2021 06:34:12 Task [ 0] updates[ 28000] train loss[0.63738] remaining[0:48:12]
01/10/2021 06:35:20 Task [ 0] updates[ 28500] train loss[0.63511] remaining[0:47:03]
01/10/2021 06:36:28 Task [ 0] updates[ 29000] train loss[0.63339] remaining[0:45:54]
01/10/2021 06:37:37 Task [ 0] updates[ 29500] train loss[0.63175] remaining[0:44:45]
01/10/2021 06:38:44 Task [ 0] updates[ 30000] train loss[0.62988] remaining[0:43:36]
01/10/2021 06:39:52 Task [ 0] updates[ 30500] train loss[0.62835] remaining[0:42:27]
01/10/2021 06:41:00 Task [ 0] updates[ 31000] train loss[0.62679] remaining[0:41:18]
01/10/2021 06:42:08 Task [ 0] updates[ 31500] train loss[0.62456] remaining[0:40:09]
01/10/2021 06:43:17 Task [ 0] updates[ 32000] train loss[0.62262] remaining[0:39:01]
01/10/2021 06:44:24 Task [ 0] updates[ 32500] train loss[0.62102] remaining[0:37:52]
01/10/2021 06:45:30 Task [ 0] updates[ 33000] train loss[0.61946] remaining[0:36:42]
01/10/2021 06:46:38 Task [ 0] updates[ 33500] train loss[0.61782] remaining[0:35:33]
01/10/2021 06:47:44 Task [ 0] updates[ 34000] train loss[0.61643] remaining[0:34:24]
01/10/2021 06:48:49 Task [ 0] updates[ 34500] train loss[0.61521] remaining[0:33:14]
01/10/2021 06:49:56 Task [ 0] updates[ 35000] train loss[0.61379] remaining[0:32:05]
01/10/2021 06:51:02 Task [ 0] updates[ 35500] train loss[0.61212] remaining[0:30:56]
01/10/2021 06:52:10 Task [ 0] updates[ 36000] train loss[0.61065] remaining[0:29:47]
01/10/2021 06:53:16 Task [ 0] updates[ 36500] train loss[0.60918] remaining[0:28:38]
01/10/2021 06:54:23 Task [ 0] updates[ 37000] train loss[0.60795] remaining[0:27:30]
01/10/2021 06:55:29 Task [ 0] updates[ 37500] train loss[0.60690] remaining[0:26:21]
01/10/2021 06:56:35 Task [ 0] updates[ 38000] train loss[0.60573] remaining[0:25:12]
01/10/2021 06:57:42 Task [ 0] updates[ 38500] train loss[0.60428] remaining[0:24:03]
01/10/2021 06:58:49 Task [ 0] updates[ 39000] train loss[0.60308] remaining[0:22:55]
01/10/2021 06:59:56 Task [ 0] updates[ 39500] train loss[0.60202] remaining[0:21:46]
01/10/2021 07:01:03 Task [ 0] updates[ 40000] train loss[0.60059] remaining[0:20:38]
01/10/2021 07:02:11 Task [ 0] updates[ 40500] train loss[0.59937] remaining[0:19:30]
01/10/2021 07:03:18 Task [ 0] updates[ 41000] train loss[0.59838] remaining[0:18:21]
01/10/2021 07:04:25 Task [ 0] updates[ 41500] train loss[0.59718] remaining[0:17:13]
01/10/2021 07:05:32 Task [ 0] updates[ 42000] train loss[0.59587] remaining[0:16:05]
01/10/2021 07:06:39 Task [ 0] updates[ 42500] train loss[0.59486] remaining[0:14:57]
01/10/2021 07:07:45 Task [ 0] updates[ 43000] train loss[0.59377] remaining[0:13:48]
01/10/2021 07:08:53 Task [ 0] updates[ 43500] train loss[0.59306] remaining[0:12:40]
01/10/2021 07:10:00 Task [ 0] updates[ 44000] train loss[0.59216] remaining[0:11:32]
01/10/2021 07:11:08 Task [ 0] updates[ 44500] train loss[0.59098] remaining[0:10:24]
01/10/2021 07:12:16 Task [ 0] updates[ 45000] train loss[0.59028] remaining[0:09:16]
01/10/2021 07:13:25 Task [ 0] updates[ 45500] train loss[0.58934] remaining[0:08:08]
01/10/2021 07:14:33 Task [ 0] updates[ 46000] train loss[0.58844] remaining[0:07:00]
01/10/2021 07:15:42 Task [ 0] updates[ 46500] train loss[0.58733] remaining[0:05:52]
01/10/2021 07:16:50 Task [ 0] updates[ 47000] train loss[0.58649] remaining[0:04:44]
01/10/2021 07:17:58 Task [ 0] updates[ 47500] train loss[0.58541] remaining[0:03:36]
01/10/2021 07:19:06 Task [ 0] updates[ 48000] train loss[0.58442] remaining[0:02:28]
01/10/2021 07:20:13 Task [ 0] updates[ 48500] train loss[0.58342] remaining[0:01:20]
01/10/2021 07:21:22 Task [ 0] updates[ 49000] train loss[0.58270] remaining[0:00:11]
01/10/2021 07:21:39 At epoch 1
01/10/2021 07:22:35 Task [ 0] updates[ 49500] train loss[0.58179] remaining[1:51:16]
01/10/2021 07:23:42 Task [ 0] updates[ 50000] train loss[0.58086] remaining[1:48:40]
01/10/2021 07:24:49 Task [ 0] updates[ 50500] train loss[0.58008] remaining[1:47:00]
01/10/2021 07:25:56 Task [ 0] updates[ 51000] train loss[0.57929] remaining[1:45:53]
01/10/2021 07:27:04 Task [ 0] updates[ 51500] train loss[0.57825] remaining[1:44:49]
01/10/2021 07:28:11 Task [ 0] updates[ 52000] train loss[0.57728] remaining[1:43:46]
01/10/2021 07:29:18 Task [ 0] updates[ 52500] train loss[0.57641] remaining[1:42:32]
01/10/2021 07:30:25 Task [ 0] updates[ 53000] train loss[0.57563] remaining[1:41:15]
01/10/2021 07:31:33 Task [ 0] updates[ 53500] train loss[0.57486] remaining[1:40:12]
01/10/2021 07:32:40 Task [ 0] updates[ 54000] train loss[0.57417] remaining[1:39:04]
01/10/2021 07:33:48 Task [ 0] updates[ 54500] train loss[0.57336] remaining[1:38:04]
01/10/2021 07:34:55 Task [ 0] updates[ 55000] train loss[0.57256] remaining[1:36:55]
01/10/2021 07:36:03 Task [ 0] updates[ 55500] train loss[0.57165] remaining[1:35:51]
01/10/2021 07:37:11 Task [ 0] updates[ 56000] train loss[0.57087] remaining[1:34:49]
01/10/2021 07:38:19 Task [ 0] updates[ 56500] train loss[0.57014] remaining[1:33:46]
01/10/2021 07:39:27 Task [ 0] updates[ 57000] train loss[0.56942] remaining[1:32:41]
01/10/2021 07:40:36 Task [ 0] updates[ 57500] train loss[0.56852] remaining[1:31:37]
01/10/2021 07:41:42 Task [ 0] updates[ 58000] train loss[0.56759] remaining[1:30:23]
01/10/2021 07:42:50 Task [ 0] updates[ 58500] train loss[0.56675] remaining[1:29:17]
01/10/2021 07:43:59 Task [ 0] updates[ 59000] train loss[0.56590] remaining[1:28:16]
01/10/2021 07:45:06 Task [ 0] updates[ 59500] train loss[0.56514] remaining[1:27:08]
01/10/2021 07:46:15 Task [ 0] updates[ 60000] train loss[0.56427] remaining[1:26:05]
01/10/2021 07:47:24 Task [ 0] updates[ 60500] train loss[0.56352] remaining[1:25:01]
01/10/2021 07:48:31 Task [ 0] updates[ 61000] train loss[0.56277] remaining[1:23:51]
01/10/2021 07:49:40 Task [ 0] updates[ 61500] train loss[0.56211] remaining[1:22:47]
01/10/2021 07:50:48 Task [ 0] updates[ 62000] train loss[0.56143] remaining[1:21:41]
01/10/2021 07:51:58 Task [ 0] updates[ 62500] train loss[0.56061] remaining[1:20:38]
01/10/2021 07:53:05 Task [ 0] updates[ 63000] train loss[0.55985] remaining[1:19:29]
01/10/2021 07:54:14 Task [ 0] updates[ 63500] train loss[0.55918] remaining[1:18:24]
01/10/2021 07:55:21 Task [ 0] updates[ 64000] train loss[0.55837] remaining[1:17:15]
01/10/2021 07:56:30 Task [ 0] updates[ 64500] train loss[0.55760] remaining[1:16:08]
01/10/2021 07:57:39 Task [ 0] updates[ 65000] train loss[0.55699] remaining[1:15:03]
01/10/2021 07:58:47 Task [ 0] updates[ 65500] train loss[0.55612] remaining[1:13:55]
01/10/2021 07:59:54 Task [ 0] updates[ 66000] train loss[0.55543] remaining[1:12:46]
01/10/2021 08:01:00 Task [ 0] updates[ 66500] train loss[0.55469] remaining[1:11:35]
01/10/2021 08:02:06 Task [ 0] updates[ 67000] train loss[0.55389] remaining[1:10:24]
01/10/2021 08:03:13 Task [ 0] updates[ 67500] train loss[0.55322] remaining[1:09:15]
01/10/2021 08:04:20 Task [ 0] updates[ 68000] train loss[0.55235] remaining[1:08:05]
01/10/2021 08:05:26 Task [ 0] updates[ 68500] train loss[0.55156] remaining[1:06:56]
01/10/2021 08:06:33 Task [ 0] updates[ 69000] train loss[0.55084] remaining[1:05:47]
01/10/2021 08:07:40 Task [ 0] updates[ 69500] train loss[0.55005] remaining[1:04:39]
01/10/2021 08:08:47 Task [ 0] updates[ 70000] train loss[0.54933] remaining[1:03:30]
01/10/2021 08:09:53 Task [ 0] updates[ 70500] train loss[0.54853] remaining[1:02:20]
01/10/2021 08:11:01 Task [ 0] updates[ 71000] train loss[0.54778] remaining[1:01:14]
01/10/2021 08:12:09 Task [ 0] updates[ 71500] train loss[0.54704] remaining[1:00:07]
01/10/2021 08:13:17 Task [ 0] updates[ 72000] train loss[0.54631] remaining[0:58:59]
01/10/2021 08:14:24 Task [ 0] updates[ 72500] train loss[0.54559] remaining[0:57:50]
01/10/2021 08:15:31 Task [ 0] updates[ 73000] train loss[0.54483] remaining[0:56:42]
01/10/2021 08:16:38 Task [ 0] updates[ 73500] train loss[0.54411] remaining[0:55:34]
01/10/2021 08:17:45 Task [ 0] updates[ 74000] train loss[0.54326] remaining[0:54:26]
01/10/2021 08:18:51 Task [ 0] updates[ 74500] train loss[0.54245] remaining[0:53:18]
01/10/2021 08:19:59 Task [ 0] updates[ 75000] train loss[0.54165] remaining[0:52:10]
01/10/2021 08:21:07 Task [ 0] updates[ 75500] train loss[0.54090] remaining[0:51:03]
01/10/2021 08:22:12 Task [ 0] updates[ 76000] train loss[0.54015] remaining[0:49:54]
01/10/2021 08:23:20 Task [ 0] updates[ 76500] train loss[0.53947] remaining[0:48:47]
01/10/2021 08:24:29 Task [ 0] updates[ 77000] train loss[0.53873] remaining[0:47:40]
01/10/2021 08:25:35 Task [ 0] updates[ 77500] train loss[0.53800] remaining[0:46:31]
01/10/2021 08:26:42 Task [ 0] updates[ 78000] train loss[0.53724] remaining[0:45:23]
01/10/2021 08:27:48 Task [ 0] updates[ 78500] train loss[0.53652] remaining[0:44:15]
01/10/2021 08:28:56 Task [ 0] updates[ 79000] train loss[0.53576] remaining[0:43:08]
01/10/2021 08:30:03 Task [ 0] updates[ 79500] train loss[0.53525] remaining[0:42:00]
01/10/2021 08:31:10 Task [ 0] updates[ 80000] train loss[0.53454] remaining[0:40:52]
01/10/2021 08:32:17 Task [ 0] updates[ 80500] train loss[0.53367] remaining[0:39:45]
01/10/2021 08:33:24 Task [ 0] updates[ 81000] train loss[0.53295] remaining[0:38:37]
01/10/2021 08:34:30 Task [ 0] updates[ 81500] train loss[0.53235] remaining[0:37:29]
01/10/2021 08:35:37 Task [ 0] updates[ 82000] train loss[0.53159] remaining[0:36:21]
01/10/2021 08:36:43 Task [ 0] updates[ 82500] train loss[0.53087] remaining[0:35:13]
01/10/2021 08:37:50 Task [ 0] updates[ 83000] train loss[0.53022] remaining[0:34:05]
01/10/2021 08:38:55 Task [ 0] updates[ 83500] train loss[0.52957] remaining[0:32:57]
01/10/2021 08:40:01 Task [ 0] updates[ 84000] train loss[0.52890] remaining[0:31:49]
01/10/2021 08:41:08 Task [ 0] updates[ 84500] train loss[0.52822] remaining[0:30:41]
01/10/2021 08:42:14 Task [ 0] updates[ 85000] train loss[0.52751] remaining[0:29:34]
01/10/2021 08:43:22 Task [ 0] updates[ 85500] train loss[0.52682] remaining[0:28:26]
01/10/2021 08:44:29 Task [ 0] updates[ 86000] train loss[0.52613] remaining[0:27:19]
01/10/2021 08:45:36 Task [ 0] updates[ 86500] train loss[0.52554] remaining[0:26:12]
01/10/2021 08:46:43 Task [ 0] updates[ 87000] train loss[0.52486] remaining[0:25:04]
01/10/2021 08:47:52 Task [ 0] updates[ 87500] train loss[0.52407] remaining[0:23:57]
01/10/2021 08:48:57 Task [ 0] updates[ 88000] train loss[0.52337] remaining[0:22:49]
01/10/2021 08:50:03 Task [ 0] updates[ 88500] train loss[0.52288] remaining[0:21:42]
01/10/2021 08:51:09 Task [ 0] updates[ 89000] train loss[0.52220] remaining[0:20:34]
01/10/2021 08:52:14 Task [ 0] updates[ 89500] train loss[0.52143] remaining[0:19:26]
01/10/2021 08:53:18 Task [ 0] updates[ 90000] train loss[0.52080] remaining[0:18:19]
01/10/2021 08:54:23 Task [ 0] updates[ 90500] train loss[0.52019] remaining[0:17:11]
01/10/2021 08:55:27 Task [ 0] updates[ 91000] train loss[0.51946] remaining[0:16:03]
01/10/2021 08:56:32 Task [ 0] updates[ 91500] train loss[0.51884] remaining[0:14:56]
01/10/2021 08:57:37 Task [ 0] updates[ 92000] train loss[0.51818] remaining[0:13:48]
01/10/2021 08:58:42 Task [ 0] updates[ 92500] train loss[0.51767] remaining[0:12:41]
01/10/2021 08:59:47 Task [ 0] updates[ 93000] train loss[0.51710] remaining[0:11:34]
01/10/2021 09:00:53 Task [ 0] updates[ 93500] train loss[0.51647] remaining[0:10:26]
01/10/2021 09:01:59 Task [ 0] updates[ 94000] train loss[0.51591] remaining[0:09:19]
01/10/2021 09:03:06 Task [ 0] updates[ 94500] train loss[0.51524] remaining[0:08:12]
01/10/2021 09:04:12 Task [ 0] updates[ 95000] train loss[0.51464] remaining[0:07:05]
01/10/2021 09:05:17 Task [ 0] updates[ 95500] train loss[0.51401] remaining[0:05:58]
01/10/2021 09:06:24 Task [ 0] updates[ 96000] train loss[0.51338] remaining[0:04:51]
01/10/2021 09:07:29 Task [ 0] updates[ 96500] train loss[0.51280] remaining[0:03:44]
01/10/2021 09:08:34 Task [ 0] updates[ 97000] train loss[0.51212] remaining[0:02:37]
01/10/2021 09:09:40 Task [ 0] updates[ 97500] train loss[0.51141] remaining[0:01:30]
01/10/2021 09:10:45 Task [ 0] updates[ 98000] train loss[0.51068] remaining[0:00:23]
01/10/2021 09:11:14 At epoch 2
01/10/2021 09:11:58 Task [ 0] updates[ 98500] train loss[0.51016] remaining[1:49:52]
01/10/2021 09:13:04 Task [ 0] updates[ 99000] train loss[0.50947] remaining[1:47:41]
01/10/2021 09:14:11 Task [ 0] updates[ 99500] train loss[0.50893] remaining[1:46:22]
01/10/2021 09:15:17 Task [ 0] updates[100000] train loss[0.50834] remaining[1:44:51]
01/10/2021 09:16:24 Task [ 0] updates[100500] train loss[0.50773] remaining[1:44:03]
01/10/2021 09:17:31 Task [ 0] updates[101000] train loss[0.50706] remaining[1:42:57]
01/10/2021 09:18:36 Task [ 0] updates[101500] train loss[0.50658] remaining[1:41:26]
01/10/2021 09:19:41 Task [ 0] updates[102000] train loss[0.50611] remaining[1:40:05]
01/10/2021 09:20:47 Task [ 0] updates[102500] train loss[0.50561] remaining[1:38:55]
01/10/2021 09:21:54 Task [ 0] updates[103000] train loss[0.50515] remaining[1:37:53]
01/10/2021 09:22:59 Task [ 0] updates[103500] train loss[0.50454] remaining[1:36:39]
01/10/2021 09:24:05 Task [ 0] updates[104000] train loss[0.50400] remaining[1:35:25]
01/10/2021 09:25:10 Task [ 0] updates[104500] train loss[0.50342] remaining[1:34:15]
01/10/2021 09:26:17 Task [ 0] updates[105000] train loss[0.50276] remaining[1:33:14]
01/10/2021 09:27:24 Task [ 0] updates[105500] train loss[0.50222] remaining[1:32:10]
01/10/2021 09:28:30 Task [ 0] updates[106000] train loss[0.50170] remaining[1:31:01]
01/10/2021 09:29:35 Task [ 0] updates[106500] train loss[0.50115] remaining[1:29:50]
01/10/2021 09:30:41 Task [ 0] updates[107000] train loss[0.50057] remaining[1:28:43]
01/10/2021 09:31:46 Task [ 0] updates[107500] train loss[0.50007] remaining[1:27:35]
01/10/2021 09:32:53 Task [ 0] updates[108000] train loss[0.49953] remaining[1:26:32]
01/10/2021 09:33:59 Task [ 0] updates[108500] train loss[0.49903] remaining[1:25:24]
01/10/2021 09:35:05 Task [ 0] updates[109000] train loss[0.49837] remaining[1:24:20]
01/10/2021 09:36:12 Task [ 0] updates[109500] train loss[0.49786] remaining[1:23:15]
01/10/2021 09:37:17 Task [ 0] updates[110000] train loss[0.49732] remaining[1:22:06]
01/10/2021 09:38:23 Task [ 0] updates[110500] train loss[0.49684] remaining[1:21:00]
01/10/2021 09:39:29 Task [ 0] updates[111000] train loss[0.49624] remaining[1:19:54]
01/10/2021 09:40:35 Task [ 0] updates[111500] train loss[0.49566] remaining[1:18:47]
01/10/2021 09:41:40 Task [ 0] updates[112000] train loss[0.49518] remaining[1:17:38]
01/10/2021 09:42:44 Task [ 0] updates[112500] train loss[0.49463] remaining[1:16:27]
01/10/2021 09:43:49 Task [ 0] updates[113000] train loss[0.49413] remaining[1:15:18]
01/10/2021 09:44:56 Task [ 0] updates[113500] train loss[0.49359] remaining[1:14:15]
01/10/2021 09:46:03 Task [ 0] updates[114000] train loss[0.49311] remaining[1:13:11]
01/10/2021 09:47:10 Task [ 0] updates[114500] train loss[0.49266] remaining[1:12:06]
01/10/2021 09:48:15 Task [ 0] updates[115000] train loss[0.49223] remaining[1:10:59]
01/10/2021 09:49:20 Task [ 0] updates[115500] train loss[0.49177] remaining[1:09:51]
01/10/2021 09:50:25 Task [ 0] updates[116000] train loss[0.49122] remaining[1:08:43]
01/10/2021 09:51:31 Task [ 0] updates[116500] train loss[0.49075] remaining[1:07:37]
01/10/2021 09:52:36 Task [ 0] updates[117000] train loss[0.49016] remaining[1:06:30]
01/10/2021 09:53:41 Task [ 0] updates[117500] train loss[0.48960] remaining[1:05:23]
01/10/2021 09:54:48 Task [ 0] updates[118000] train loss[0.48908] remaining[1:04:18]
01/10/2021 09:55:54 Task [ 0] updates[118500] train loss[0.48853] remaining[1:03:12]
01/10/2021 09:57:00 Task [ 0] updates[119000] train loss[0.48804] remaining[1:02:06]
01/10/2021 09:58:05 Task [ 0] updates[119500] train loss[0.48757] remaining[1:00:59]
01/10/2021 09:59:11 Task [ 0] updates[120000] train loss[0.48708] remaining[0:59:53]
01/10/2021 10:00:16 Task [ 0] updates[120500] train loss[0.48661] remaining[0:58:47]
01/10/2021 10:01:22 Task [ 0] updates[121000] train loss[0.48617] remaining[0:57:41]
01/10/2021 10:02:27 Task [ 0] updates[121500] train loss[0.48572] remaining[0:56:34]
01/10/2021 10:03:33 Task [ 0] updates[122000] train loss[0.48525] remaining[0:55:28]
01/10/2021 10:04:38 Task [ 0] updates[122500] train loss[0.48476] remaining[0:54:22]
01/10/2021 10:05:44 Task [ 0] updates[123000] train loss[0.48424] remaining[0:53:16]
01/10/2021 10:06:49 Task [ 0] updates[123500] train loss[0.48368] remaining[0:52:09]
01/10/2021 10:07:54 Task [ 0] updates[124000] train loss[0.48322] remaining[0:51:03]
01/10/2021 10:09:00 Task [ 0] updates[124500] train loss[0.48267] remaining[0:49:56]
01/10/2021 10:10:05 Task [ 0] updates[125000] train loss[0.48215] remaining[0:48:51]
01/10/2021 10:11:11 Task [ 0] updates[125500] train loss[0.48172] remaining[0:47:45]
01/10/2021 10:12:17 Task [ 0] updates[126000] train loss[0.48126] remaining[0:46:39]
01/10/2021 10:13:22 Task [ 0] updates[126500] train loss[0.48075] remaining[0:45:32]
01/10/2021 10:14:27 Task [ 0] updates[127000] train loss[0.48026] remaining[0:44:26]
01/10/2021 10:15:33 Task [ 0] updates[127500] train loss[0.47973] remaining[0:43:20]
01/10/2021 10:16:37 Task [ 0] updates[128000] train loss[0.47919] remaining[0:42:13]
01/10/2021 10:17:43 Task [ 0] updates[128500] train loss[0.47882] remaining[0:41:08]
01/10/2021 10:18:48 Task [ 0] updates[129000] train loss[0.47834] remaining[0:40:02]
01/10/2021 10:19:54 Task [ 0] updates[129500] train loss[0.47779] remaining[0:38:56]
01/10/2021 10:21:00 Task [ 0] updates[130000] train loss[0.47745] remaining[0:37:50]
01/10/2021 10:22:05 Task [ 0] updates[130500] train loss[0.47693] remaining[0:36:44]
01/10/2021 10:23:11 Task [ 0] updates[131000] train loss[0.47648] remaining[0:35:39]
01/10/2021 10:24:17 Task [ 0] updates[131500] train loss[0.47591] remaining[0:34:33]
01/10/2021 10:25:24 Task [ 0] updates[132000] train loss[0.47555] remaining[0:33:28]
01/10/2021 10:26:31 Task [ 0] updates[132500] train loss[0.47503] remaining[0:32:22]
01/10/2021 10:27:38 Task [ 0] updates[133000] train loss[0.47453] remaining[0:31:17]
01/10/2021 10:28:43 Task [ 0] updates[133500] train loss[0.47408] remaining[0:30:11]
01/10/2021 10:29:48 Task [ 0] updates[134000] train loss[0.47361] remaining[0:29:05]
01/10/2021 10:30:56 Task [ 0] updates[134500] train loss[0.47316] remaining[0:28:00]
01/10/2021 10:32:04 Task [ 0] updates[135000] train loss[0.47263] remaining[0:26:55]
01/10/2021 10:33:12 Task [ 0] updates[135500] train loss[0.47219] remaining[0:25:50]
01/10/2021 10:34:20 Task [ 0] updates[136000] train loss[0.47174] remaining[0:24:44]
01/10/2021 10:35:30 Task [ 0] updates[136500] train loss[0.47120] remaining[0:23:40]
01/10/2021 10:36:40 Task [ 0] updates[137000] train loss[0.47079] remaining[0:22:35]
01/10/2021 10:37:51 Task [ 0] updates[137500] train loss[0.47027] remaining[0:21:30]
01/10/2021 10:38:58 Task [ 0] updates[138000] train loss[0.46984] remaining[0:20:24]
01/10/2021 10:40:02 Task [ 0] updates[138500] train loss[0.46938] remaining[0:19:17]
01/10/2021 10:41:06 Task [ 0] updates[139000] train loss[0.46889] remaining[0:18:11]
01/10/2021 10:42:12 Task [ 0] updates[139500] train loss[0.46848] remaining[0:17:05]
01/10/2021 10:43:22 Task [ 0] updates[140000] train loss[0.46796] remaining[0:16:00]
01/10/2021 10:44:30 Task [ 0] updates[140500] train loss[0.46754] remaining[0:14:54]
01/10/2021 10:45:35 Task [ 0] updates[141000] train loss[0.46700] remaining[0:13:48]
01/10/2021 10:46:42 Task [ 0] updates[141500] train loss[0.46658] remaining[0:12:42]
01/10/2021 10:47:48 Task [ 0] updates[142000] train loss[0.46613] remaining[0:11:35]
01/10/2021 10:48:55 Task [ 0] updates[142500] train loss[0.46565] remaining[0:10:29]
01/10/2021 10:50:02 Task [ 0] updates[143000] train loss[0.46526] remaining[0:09:23]
01/10/2021 10:51:09 Task [ 0] updates[143500] train loss[0.46484] remaining[0:08:17]
01/10/2021 10:52:12 Task [ 0] updates[144000] train loss[0.46441] remaining[0:07:11]
01/10/2021 10:53:16 Task [ 0] updates[144500] train loss[0.46393] remaining[0:06:05]
01/10/2021 10:54:24 Task [ 0] updates[145000] train loss[0.46355] remaining[0:04:59]
01/10/2021 10:55:28 Task [ 0] updates[145500] train loss[0.46306] remaining[0:03:53]
01/10/2021 10:56:33 Task [ 0] updates[146000] train loss[0.46266] remaining[0:02:47]
01/10/2021 10:57:38 Task [ 0] updates[146500] train loss[0.46220] remaining[0:01:40]
01/10/2021 10:58:43 Task [ 0] updates[147000] train loss[0.46173] remaining[0:00:34]
01/10/2021 10:59:22 At epoch 3
01/10/2021 10:59:53 Task [ 0] updates[147500] train loss[0.46136] remaining[1:46:04]
01/10/2021 11:00:59 Task [ 0] updates[148000] train loss[0.46092] remaining[1:45:30]
01/10/2021 11:02:04 Task [ 0] updates[148500] train loss[0.46058] remaining[1:44:07]
01/10/2021 11:03:10 Task [ 0] updates[149000] train loss[0.46015] remaining[1:43:32]
01/10/2021 11:04:17 Task [ 0] updates[149500] train loss[0.45974] remaining[1:42:57]
01/10/2021 11:05:23 Task [ 0] updates[150000] train loss[0.45922] remaining[1:41:56]
01/10/2021 11:06:31 Task [ 0] updates[150500] train loss[0.45885] remaining[1:41:15]
01/10/2021 11:07:39 Task [ 0] updates[151000] train loss[0.45851] remaining[1:40:27]
01/10/2021 11:08:49 Task [ 0] updates[151500] train loss[0.45810] remaining[1:39:55]
01/10/2021 11:09:59 Task [ 0] updates[152000] train loss[0.45776] remaining[1:39:26]
01/10/2021 11:11:11 Task [ 0] updates[152500] train loss[0.45738] remaining[1:38:51]
01/10/2021 11:12:19 Task [ 0] updates[153000] train loss[0.45698] remaining[1:37:48]
01/10/2021 11:13:28 Task [ 0] updates[153500] train loss[0.45655] remaining[1:36:54]
01/10/2021 11:14:39 Task [ 0] updates[154000] train loss[0.45608] remaining[1:36:01]
01/10/2021 11:15:50 Task [ 0] updates[154500] train loss[0.45568] remaining[1:35:12]
01/10/2021 11:17:00 Task [ 0] updates[155000] train loss[0.45522] remaining[1:34:14]
01/10/2021 11:18:12 Task [ 0] updates[155500] train loss[0.45478] remaining[1:33:21]
01/10/2021 11:19:22 Task [ 0] updates[156000] train loss[0.45437] remaining[1:32:20]
01/10/2021 11:20:37 Task [ 0] updates[156500] train loss[0.45390] remaining[1:31:38]
01/10/2021 11:21:53 Task [ 0] updates[157000] train loss[0.45357] remaining[1:31:01]
01/10/2021 11:23:04 Task [ 0] updates[157500] train loss[0.45317] remaining[1:29:56]
01/10/2021 11:24:16 Task [ 0] updates[158000] train loss[0.45273] remaining[1:28:54]
01/10/2021 11:25:25 Task [ 0] updates[158500] train loss[0.45232] remaining[1:27:43]
01/10/2021 11:26:34 Task [ 0] updates[159000] train loss[0.45195] remaining[1:26:31]
01/10/2021 11:27:46 Task [ 0] updates[159500] train loss[0.45155] remaining[1:25:30]
01/10/2021 11:28:58 Task [ 0] updates[160000] train loss[0.45115] remaining[1:24:28]
01/10/2021 11:30:09 Task [ 0] updates[160500] train loss[0.45069] remaining[1:23:23]
01/10/2021 11:31:20 Task [ 0] updates[161000] train loss[0.45027] remaining[1:22:15]
01/10/2021 11:32:28 Task [ 0] updates[161500] train loss[0.44982] remaining[1:21:00]
01/10/2021 11:33:39 Task [ 0] updates[162000] train loss[0.44943] remaining[1:19:54]
01/10/2021 11:34:49 Task [ 0] updates[162500] train loss[0.44899] remaining[1:18:44]
01/10/2021 11:35:59 Task [ 0] updates[163000] train loss[0.44858] remaining[1:17:36]
01/10/2021 11:37:11 Task [ 0] updates[163500] train loss[0.44821] remaining[1:16:31]
01/10/2021 11:38:19 Task [ 0] updates[164000] train loss[0.44786] remaining[1:15:16]
01/10/2021 11:39:25 Task [ 0] updates[164500] train loss[0.44749] remaining[1:14:00]
01/10/2021 11:40:35 Task [ 0] updates[165000] train loss[0.44700] remaining[1:12:51]
01/10/2021 11:41:44 Task [ 0] updates[165500] train loss[0.44667] remaining[1:11:40]
01/10/2021 11:42:54 Task [ 0] updates[166000] train loss[0.44618] remaining[1:10:31]
01/10/2021 11:44:07 Task [ 0] updates[166500] train loss[0.44577] remaining[1:09:26]
01/10/2021 11:45:17 Task [ 0] updates[167000] train loss[0.44537] remaining[1:08:17]
01/10/2021 11:46:29 Task [ 0] updates[167500] train loss[0.44502] remaining[1:07:10]
01/10/2021 11:47:42 Task [ 0] updates[168000] train loss[0.44459] remaining[1:06:04]
01/10/2021 11:48:54 Task [ 0] updates[168500] train loss[0.44418] remaining[1:04:57]
01/10/2021 11:50:04 Task [ 0] updates[169000] train loss[0.44381] remaining[1:03:48]
01/10/2021 11:51:18 Task [ 0] updates[169500] train loss[0.44336] remaining[1:02:42]
01/10/2021 11:52:29 Task [ 0] updates[170000] train loss[0.44292] remaining[1:01:33]
01/10/2021 11:53:40 Task [ 0] updates[170500] train loss[0.44259] remaining[1:00:24]
01/10/2021 11:54:51 Task [ 0] updates[171000] train loss[0.44216] remaining[0:59:15]
01/10/2021 11:56:05 Task [ 0] updates[171500] train loss[0.44180] remaining[0:58:08]
01/10/2021 11:57:17 Task [ 0] updates[172000] train loss[0.44139] remaining[0:57:00]
01/10/2021 11:58:29 Task [ 0] updates[172500] train loss[0.44099] remaining[0:55:51]
01/10/2021 11:59:42 Task [ 0] updates[173000] train loss[0.44061] remaining[0:54:44]
01/10/2021 12:00:55 Task [ 0] updates[173500] train loss[0.44015] remaining[0:53:36]
01/10/2021 12:02:05 Task [ 0] updates[174000] train loss[0.43973] remaining[0:52:25]
01/10/2021 12:03:16 Task [ 0] updates[174500] train loss[0.43933] remaining[0:51:15]
01/10/2021 12:04:27 Task [ 0] updates[175000] train loss[0.43898] remaining[0:50:06]
01/10/2021 12:05:42 Task [ 0] updates[175500] train loss[0.43850] remaining[0:48:59]
01/10/2021 12:06:54 Task [ 0] updates[176000] train loss[0.43814] remaining[0:47:49]
01/10/2021 12:08:03 Task [ 0] updates[176500] train loss[0.43775] remaining[0:46:38]
01/10/2021 12:09:11 Task [ 0] updates[177000] train loss[0.43728] remaining[0:45:25]
01/10/2021 12:10:21 Task [ 0] updates[177500] train loss[0.43690] remaining[0:44:15]
01/10/2021 12:11:32 Task [ 0] updates[178000] train loss[0.43655] remaining[0:43:05]
01/10/2021 12:12:43 Task [ 0] updates[178500] train loss[0.43611] remaining[0:41:55]
01/10/2021 12:13:54 Task [ 0] updates[179000] train loss[0.43572] remaining[0:40:44]
01/10/2021 12:15:05 Task [ 0] updates[179500] train loss[0.43529] remaining[0:39:34]
01/10/2021 12:16:15 Task [ 0] updates[180000] train loss[0.43485] remaining[0:38:24]
01/10/2021 12:17:28 Task [ 0] updates[180500] train loss[0.43443] remaining[0:37:14]
01/10/2021 12:18:42 Task [ 0] updates[181000] train loss[0.43412] remaining[0:36:05]
01/10/2021 12:19:57 Task [ 0] updates[181500] train loss[0.43377] remaining[0:34:57]
01/10/2021 12:21:09 Task [ 0] updates[182000] train loss[0.43338] remaining[0:33:47]
01/10/2021 12:22:20 Task [ 0] updates[182500] train loss[0.43300] remaining[0:32:36]
01/10/2021 12:23:34 Task [ 0] updates[183000] train loss[0.43262] remaining[0:31:27]
01/10/2021 12:24:45 Task [ 0] updates[183500] train loss[0.43224] remaining[0:30:16]
01/10/2021 12:25:56 Task [ 0] updates[184000] train loss[0.43183] remaining[0:29:06]
01/10/2021 12:27:07 Task [ 0] updates[184500] train loss[0.43146] remaining[0:27:55]
01/10/2021 12:28:17 Task [ 0] updates[185000] train loss[0.43110] remaining[0:26:44]
01/10/2021 12:29:27 Task [ 0] updates[185500] train loss[0.43064] remaining[0:25:33]
01/10/2021 12:30:38 Task [ 0] updates[186000] train loss[0.43028] remaining[0:24:23]
01/10/2021 12:31:49 Task [ 0] updates[186500] train loss[0.42989] remaining[0:23:12]
01/10/2021 12:33:00 Task [ 0] updates[187000] train loss[0.42947] remaining[0:22:02]
01/10/2021 12:34:11 Task [ 0] updates[187500] train loss[0.42905] remaining[0:20:51]
01/10/2021 12:35:22 Task [ 0] updates[188000] train loss[0.42865] remaining[0:19:40]
01/10/2021 12:36:33 Task [ 0] updates[188500] train loss[0.42826] remaining[0:18:30]
01/10/2021 12:37:44 Task [ 0] updates[189000] train loss[0.42782] remaining[0:17:19]
01/10/2021 12:38:55 Task [ 0] updates[189500] train loss[0.42746] remaining[0:16:08]
01/10/2021 12:40:05 Task [ 0] updates[190000] train loss[0.42704] remaining[0:14:58]
01/10/2021 12:41:16 Task [ 0] updates[190500] train loss[0.42672] remaining[0:13:47]
01/10/2021 12:42:26 Task [ 0] updates[191000] train loss[0.42632] remaining[0:12:36]
01/10/2021 12:43:37 Task [ 0] updates[191500] train loss[0.42595] remaining[0:11:26]
01/10/2021 12:44:47 Task [ 0] updates[192000] train loss[0.42560] remaining[0:10:15]
01/10/2021 12:46:03 Task [ 0] updates[192500] train loss[0.42521] remaining[0:09:05]
01/10/2021 12:47:12 Task [ 0] updates[193000] train loss[0.42482] remaining[0:07:54]
01/10/2021 12:48:21 Task [ 0] updates[193500] train loss[0.42440] remaining[0:06:43]
01/10/2021 12:49:32 Task [ 0] updates[194000] train loss[0.42406] remaining[0:05:32]
01/10/2021 12:50:42 Task [ 0] updates[194500] train loss[0.42366] remaining[0:04:21]
01/10/2021 12:51:53 Task [ 0] updates[195000] train loss[0.42330] remaining[0:03:11]
01/10/2021 12:53:04 Task [ 0] updates[195500] train loss[0.42291] remaining[0:02:00]
01/10/2021 12:54:15 Task [ 0] updates[196000] train loss[0.42249] remaining[0:00:49]
01/10/2021 12:55:10 At epoch 4
01/10/2021 12:55:31 Task [ 0] updates[196500] train loss[0.42217] remaining[1:54:39]
01/10/2021 12:56:41 Task [ 0] updates[197000] train loss[0.42181] remaining[1:52:32]
01/10/2021 12:57:51 Task [ 0] updates[197500] train loss[0.42145] remaining[1:51:59]
01/10/2021 12:59:01 Task [ 0] updates[198000] train loss[0.42111] remaining[1:50:52]
01/10/2021 01:00:13 Task [ 0] updates[198500] train loss[0.42074] remaining[1:50:23]
01/10/2021 01:01:24 Task [ 0] updates[199000] train loss[0.42037] remaining[1:49:09]
01/10/2021 01:02:34 Task [ 0] updates[199500] train loss[0.41998] remaining[1:47:48]
01/10/2021 01:03:43 Task [ 0] updates[200000] train loss[0.41961] remaining[1:46:26]
01/10/2021 01:04:50 Task [ 0] updates[200500] train loss[0.41929] remaining[1:44:44]
01/10/2021 01:06:01 Task [ 0] updates[201000] train loss[0.41896] remaining[1:43:40]
01/10/2021 01:07:12 Task [ 0] updates[201500] train loss[0.41865] remaining[1:42:42]
01/10/2021 01:08:24 Task [ 0] updates[202000] train loss[0.41830] remaining[1:41:41]
01/10/2021 01:09:36 Task [ 0] updates[202500] train loss[0.41793] remaining[1:40:47]
01/10/2021 01:10:48 Task [ 0] updates[203000] train loss[0.41754] remaining[1:39:45]
01/10/2021 01:11:59 Task [ 0] updates[203500] train loss[0.41719] remaining[1:38:39]
01/10/2021 01:13:09 Task [ 0] updates[204000] train loss[0.41681] remaining[1:37:24]
01/10/2021 01:14:18 Task [ 0] updates[204500] train loss[0.41644] remaining[1:36:03]
01/10/2021 01:15:28 Task [ 0] updates[205000] train loss[0.41607] remaining[1:34:54]
01/10/2021 01:16:41 Task [ 0] updates[205500] train loss[0.41571] remaining[1:33:54]
01/10/2021 01:17:53 Task [ 0] updates[206000] train loss[0.41542] remaining[1:32:48]
01/10/2021 01:19:03 Task [ 0] updates[206500] train loss[0.41499] remaining[1:31:37]
01/10/2021 01:20:15 Task [ 0] updates[207000] train loss[0.41462] remaining[1:30:30]
01/10/2021 01:21:27 Task [ 0] updates[207500] train loss[0.41427] remaining[1:29:24]
01/10/2021 01:22:35 Task [ 0] updates[208000] train loss[0.41395] remaining[1:28:05]
01/10/2021 01:23:45 Task [ 0] updates[208500] train loss[0.41359] remaining[1:26:53]
01/10/2021 01:24:56 Task [ 0] updates[209000] train loss[0.41322] remaining[1:25:44]
01/10/2021 01:26:08 Task [ 0] updates[209500] train loss[0.41289] remaining[1:24:38]
01/10/2021 01:27:19 Task [ 0] updates[210000] train loss[0.41253] remaining[1:23:28]
01/10/2021 01:28:30 Task [ 0] updates[210500] train loss[0.41211] remaining[1:22:17]
01/10/2021 01:29:39 Task [ 0] updates[211000] train loss[0.41178] remaining[1:21:04]
01/10/2021 01:30:50 Task [ 0] updates[211500] train loss[0.41145] remaining[1:19:53]
01/10/2021 01:31:59 Task [ 0] updates[212000] train loss[0.41106] remaining[1:18:40]
01/10/2021 01:33:10 Task [ 0] updates[212500] train loss[0.41075] remaining[1:17:29]
01/10/2021 01:34:21 Task [ 0] updates[213000] train loss[0.41039] remaining[1:16:20]
01/10/2021 01:35:32 Task [ 0] updates[213500] train loss[0.41006] remaining[1:15:10]
01/10/2021 01:36:42 Task [ 0] updates[214000] train loss[0.40968] remaining[1:13:59]
01/10/2021 01:37:53 Task [ 0] updates[214500] train loss[0.40934] remaining[1:12:49]
01/10/2021 01:39:04 Task [ 0] updates[215000] train loss[0.40892] remaining[1:11:38]
01/10/2021 01:40:12 Task [ 0] updates[215500] train loss[0.40858] remaining[1:10:23]
01/10/2021 01:41:23 Task [ 0] updates[216000] train loss[0.40819] remaining[1:09:13]
01/10/2021 01:42:33 Task [ 0] updates[216500] train loss[0.40791] remaining[1:08:03]
01/10/2021 01:43:43 Task [ 0] updates[217000] train loss[0.40751] remaining[1:06:51]
01/10/2021 01:44:53 Task [ 0] updates[217500] train loss[0.40718] remaining[1:05:40]
01/10/2021 01:46:04 Task [ 0] updates[218000] train loss[0.40686] remaining[1:04:30]
01/10/2021 01:47:16 Task [ 0] updates[218500] train loss[0.40646] remaining[1:03:21]
01/10/2021 01:48:27 Task [ 0] updates[219000] train loss[0.40611] remaining[1:02:11]
01/10/2021 01:49:37 Task [ 0] updates[219500] train loss[0.40577] remaining[1:01:00]
01/10/2021 01:50:52 Task [ 0] updates[220000] train loss[0.40540] remaining[0:59:55]
01/10/2021 01:52:06 Task [ 0] updates[220500] train loss[0.40506] remaining[0:58:47]
01/10/2021 01:53:17 Task [ 0] updates[221000] train loss[0.40478] remaining[0:57:37]
01/10/2021 01:54:29 Task [ 0] updates[221500] train loss[0.40442] remaining[0:56:27]
01/10/2021 01:55:41 Task [ 0] updates[222000] train loss[0.40412] remaining[0:55:17]
01/10/2021 01:56:51 Task [ 0] updates[222500] train loss[0.40375] remaining[0:54:07]
01/10/2021 01:58:01 Task [ 0] updates[223000] train loss[0.40336] remaining[0:52:55]
01/10/2021 01:59:12 Task [ 0] updates[223500] train loss[0.40303] remaining[0:51:44]
01/10/2021 02:00:21 Task [ 0] updates[224000] train loss[0.40272] remaining[0:50:32]
01/10/2021 02:01:33 Task [ 0] updates[224500] train loss[0.40230] remaining[0:49:22]
01/10/2021 02:02:43 Task [ 0] updates[225000] train loss[0.40200] remaining[0:48:11]
01/10/2021 02:03:56 Task [ 0] updates[225500] train loss[0.40167] remaining[0:47:02]
01/10/2021 02:05:06 Task [ 0] updates[226000] train loss[0.40129] remaining[0:45:51]
01/10/2021 02:06:18 Task [ 0] updates[226500] train loss[0.40097] remaining[0:44:41]
01/10/2021 02:07:28 Task [ 0] updates[227000] train loss[0.40066] remaining[0:43:29]
01/10/2021 02:08:39 Task [ 0] updates[227500] train loss[0.40032] remaining[0:42:19]
01/10/2021 02:09:51 Task [ 0] updates[228000] train loss[0.39999] remaining[0:41:08]
01/10/2021 02:11:02 Task [ 0] updates[228500] train loss[0.39963] remaining[0:39:58]
01/10/2021 02:12:12 Task [ 0] updates[229000] train loss[0.39930] remaining[0:38:47]
01/10/2021 02:13:23 Task [ 0] updates[229500] train loss[0.39894] remaining[0:37:36]
01/10/2021 02:14:36 Task [ 0] updates[230000] train loss[0.39861] remaining[0:36:26]
01/10/2021 02:15:45 Task [ 0] updates[230500] train loss[0.39831] remaining[0:35:15]
01/10/2021 02:16:56 Task [ 0] updates[231000] train loss[0.39796] remaining[0:34:04]
01/10/2021 02:18:09 Task [ 0] updates[231500] train loss[0.39760] remaining[0:32:54]
01/10/2021 02:19:21 Task [ 0] updates[232000] train loss[0.39721] remaining[0:31:44]
01/10/2021 02:20:32 Task [ 0] updates[232500] train loss[0.39689] remaining[0:30:33]
01/10/2021 02:21:42 Task [ 0] updates[233000] train loss[0.39655] remaining[0:29:22]
01/10/2021 02:22:53 Task [ 0] updates[233500] train loss[0.39620] remaining[0:28:11]
01/10/2021 02:24:02 Task [ 0] updates[234000] train loss[0.39591] remaining[0:27:00]
01/10/2021 02:25:11 Task [ 0] updates[234500] train loss[0.39555] remaining[0:25:48]
01/10/2021 02:26:23 Task [ 0] updates[235000] train loss[0.39513] remaining[0:24:38]
01/10/2021 02:27:34 Task [ 0] updates[235500] train loss[0.39484] remaining[0:23:27]
01/10/2021 02:28:45 Task [ 0] updates[236000] train loss[0.39449] remaining[0:22:16]
01/10/2021 02:29:58 Task [ 0] updates[236500] train loss[0.39407] remaining[0:21:06]
01/10/2021 02:31:09 Task [ 0] updates[237000] train loss[0.39373] remaining[0:19:55]
01/10/2021 02:32:19 Task [ 0] updates[237500] train loss[0.39341] remaining[0:18:44]
01/10/2021 02:33:28 Task [ 0] updates[238000] train loss[0.39301] remaining[0:17:33]
01/10/2021 02:34:38 Task [ 0] updates[238500] train loss[0.39268] remaining[0:16:22]
01/10/2021 02:35:50 Task [ 0] updates[239000] train loss[0.39229] remaining[0:15:12]
01/10/2021 02:37:03 Task [ 0] updates[239500] train loss[0.39200] remaining[0:14:01]
01/10/2021 02:38:14 Task [ 0] updates[240000] train loss[0.39164] remaining[0:12:50]
01/10/2021 02:39:25 Task [ 0] updates[240500] train loss[0.39135] remaining[0:11:39]
01/10/2021 02:40:37 Task [ 0] updates[241000] train loss[0.39097] remaining[0:10:29]
01/10/2021 02:41:47 Task [ 0] updates[241500] train loss[0.39071] remaining[0:09:18]
01/10/2021 02:42:56 Task [ 0] updates[242000] train loss[0.39036] remaining[0:08:07]
01/10/2021 02:44:09 Task [ 0] updates[242500] train loss[0.39000] remaining[0:06:56]
01/10/2021 02:45:21 Task [ 0] updates[243000] train loss[0.38964] remaining[0:05:45]
01/10/2021 02:46:31 Task [ 0] updates[243500] train loss[0.38934] remaining[0:04:34]
01/10/2021 02:47:41 Task [ 0] updates[244000] train loss[0.38900] remaining[0:03:24]
01/10/2021 02:48:51 Task [ 0] updates[244500] train loss[0.38864] remaining[0:02:13]
01/10/2021 02:50:03 Task [ 0] updates[245000] train loss[0.38828] remaining[0:01:02]
01/10/2021 02:51:08 At epoch 5
01/10/2021 02:51:16 Task [ 0] updates[245500] train loss[0.38795] remaining[1:54:08]
01/10/2021 02:52:25 Task [ 0] updates[246000] train loss[0.38763] remaining[1:52:06]
01/10/2021 02:53:35 Task [ 0] updates[246500] train loss[0.38724] remaining[1:51:15]
01/10/2021 02:54:44 Task [ 0] updates[247000] train loss[0.38697] remaining[1:49:57]
01/10/2021 02:55:56 Task [ 0] updates[247500] train loss[0.38664] remaining[1:49:28]
01/10/2021 02:57:06 Task [ 0] updates[248000] train loss[0.38631] remaining[1:48:29]
01/10/2021 02:58:16 Task [ 0] updates[248500] train loss[0.38596] remaining[1:47:16]
01/10/2021 02:59:24 Task [ 0] updates[249000] train loss[0.38563] remaining[1:45:51]
01/10/2021 03:00:33 Task [ 0] updates[249500] train loss[0.38532] remaining[1:44:22]
01/10/2021 03:01:44 Task [ 0] updates[250000] train loss[0.38502] remaining[1:43:33]
01/10/2021 03:02:55 Task [ 0] updates[250500] train loss[0.38476] remaining[1:42:34]
01/10/2021 03:04:07 Task [ 0] updates[251000] train loss[0.38447] remaining[1:41:39]
01/10/2021 03:05:17 Task [ 0] updates[251500] train loss[0.38415] remaining[1:40:28]
01/10/2021 03:06:30 Task [ 0] updates[252000] train loss[0.38377] remaining[1:39:37]
01/10/2021 03:07:41 Task [ 0] updates[252500] train loss[0.38345] remaining[1:38:31]
01/10/2021 03:08:50 Task [ 0] updates[253000] train loss[0.38313] remaining[1:37:15]
01/10/2021 03:09:59 Task [ 0] updates[253500] train loss[0.38283] remaining[1:35:59]
01/10/2021 03:11:11 Task [ 0] updates[254000] train loss[0.38248] remaining[1:34:54]
01/10/2021 03:12:23 Task [ 0] updates[254500] train loss[0.38213] remaining[1:33:54]
01/10/2021 03:13:35 Task [ 0] updates[255000] train loss[0.38179] remaining[1:32:51]
01/10/2021 03:14:46 Task [ 0] updates[255500] train loss[0.38147] remaining[1:31:39]
01/10/2021 03:15:59 Task [ 0] updates[256000] train loss[0.38117] remaining[1:30:41]
01/10/2021 03:17:08 Task [ 0] updates[256500] train loss[0.38080] remaining[1:29:25]
01/10/2021 03:18:18 Task [ 0] updates[257000] train loss[0.38054] remaining[1:28:13]
01/10/2021 03:19:30 Task [ 0] updates[257500] train loss[0.38022] remaining[1:27:04]
01/10/2021 03:20:41 Task [ 0] updates[258000] train loss[0.37994] remaining[1:25:57]
01/10/2021 03:21:51 Task [ 0] updates[258500] train loss[0.37961] remaining[1:24:44]
01/10/2021 03:23:02 Task [ 0] updates[259000] train loss[0.37929] remaining[1:23:35]
01/10/2021 03:24:12 Task [ 0] updates[259500] train loss[0.37890] remaining[1:22:22]
01/10/2021 03:25:22 Task [ 0] updates[260000] train loss[0.37859] remaining[1:21:11]
01/10/2021 03:26:29 Task [ 0] updates[260500] train loss[0.37825] remaining[1:19:52]
01/10/2021 03:27:38 Task [ 0] updates[261000] train loss[0.37793] remaining[1:18:39]
01/10/2021 03:28:49 Task [ 0] updates[261500] train loss[0.37763] remaining[1:17:30]
01/10/2021 03:29:59 Task [ 0] updates[262000] train loss[0.37733] remaining[1:16:18]
01/10/2021 03:31:10 Task [ 0] updates[262500] train loss[0.37702] remaining[1:15:10]
01/10/2021 03:32:20 Task [ 0] updates[263000] train loss[0.37667] remaining[1:13:59]
01/10/2021 03:33:30 Task [ 0] updates[263500] train loss[0.37637] remaining[1:12:47]
01/10/2021 03:34:40 Task [ 0] updates[264000] train loss[0.37602] remaining[1:11:37]
01/10/2021 03:35:51 Task [ 0] updates[264500] train loss[0.37564] remaining[1:10:27]
01/10/2021 03:37:01 Task [ 0] updates[265000] train loss[0.37537] remaining[1:09:16]
01/10/2021 03:38:13 Task [ 0] updates[265500] train loss[0.37507] remaining[1:08:07]
01/10/2021 03:39:23 Task [ 0] updates[266000] train loss[0.37475] remaining[1:06:57]
01/10/2021 03:40:33 Task [ 0] updates[266500] train loss[0.37443] remaining[1:05:46]
01/10/2021 03:41:46 Task [ 0] updates[267000] train loss[0.37411] remaining[1:04:38]
01/10/2021 03:42:57 Task [ 0] updates[267500] train loss[0.37374] remaining[1:03:29]
01/10/2021 03:44:06 Task [ 0] updates[268000] train loss[0.37341] remaining[1:02:17]
01/10/2021 03:45:16 Task [ 0] updates[268500] train loss[0.37312] remaining[1:01:06]
01/10/2021 03:46:28 Task [ 0] updates[269000] train loss[0.37282] remaining[0:59:57]
01/10/2021 03:47:39 Task [ 0] updates[269500] train loss[0.37253] remaining[0:58:47]
01/10/2021 03:48:49 Task [ 0] updates[270000] train loss[0.37226] remaining[0:57:36]
01/10/2021 03:49:58 Task [ 0] updates[270500] train loss[0.37197] remaining[0:56:25]
01/10/2021 03:51:09 Task [ 0] updates[271000] train loss[0.37167] remaining[0:55:14]
01/10/2021 03:52:20 Task [ 0] updates[271500] train loss[0.37133] remaining[0:54:04]
01/10/2021 03:53:30 Task [ 0] updates[272000] train loss[0.37097] remaining[0:52:53]
01/10/2021 03:54:41 Task [ 0] updates[272500] train loss[0.37069] remaining[0:51:43]
01/10/2021 03:55:51 Task [ 0] updates[273000] train loss[0.37039] remaining[0:50:33]
01/10/2021 03:57:01 Task [ 0] updates[273500] train loss[0.37005] remaining[0:49:22]
01/10/2021 03:58:10 Task [ 0] updates[274000] train loss[0.36973] remaining[0:48:11]
01/10/2021 03:59:21 Task [ 0] updates[274500] train loss[0.36942] remaining[0:47:00]
01/10/2021 04:00:31 Task [ 0] updates[275000] train loss[0.36910] remaining[0:45:50]
01/10/2021 04:01:39 Task [ 0] updates[275500] train loss[0.36875] remaining[0:44:38]
01/10/2021 04:02:49 Task [ 0] updates[276000] train loss[0.36846] remaining[0:43:27]
01/10/2021 04:03:59 Task [ 0] updates[276500] train loss[0.36814] remaining[0:42:17]
01/10/2021 04:05:09 Task [ 0] updates[277000] train loss[0.36780] remaining[0:41:06]
01/10/2021 04:06:19 Task [ 0] updates[277500] train loss[0.36752] remaining[0:39:56]
01/10/2021 04:07:31 Task [ 0] updates[278000] train loss[0.36717] remaining[0:38:46]
01/10/2021 04:08:41 Task [ 0] updates[278500] train loss[0.36683] remaining[0:37:35]
01/10/2021 04:09:53 Task [ 0] updates[279000] train loss[0.36651] remaining[0:36:26]
01/10/2021 04:11:04 Task [ 0] updates[279500] train loss[0.36622] remaining[0:35:16]
01/10/2021 04:12:14 Task [ 0] updates[280000] train loss[0.36587] remaining[0:34:05]
01/10/2021 04:13:24 Task [ 0] updates[280500] train loss[0.36559] remaining[0:32:54]
01/10/2021 04:14:34 Task [ 0] updates[281000] train loss[0.36526] remaining[0:31:44]
01/10/2021 04:15:46 Task [ 0] updates[281500] train loss[0.36494] remaining[0:30:34]
01/10/2021 04:16:56 Task [ 0] updates[282000] train loss[0.36465] remaining[0:29:23]
01/10/2021 04:18:05 Task [ 0] updates[282500] train loss[0.36434] remaining[0:28:13]
01/10/2021 04:19:14 Task [ 0] updates[283000] train loss[0.36410] remaining[0:27:02]
01/10/2021 04:20:24 Task [ 0] updates[283500] train loss[0.36381] remaining[0:25:52]
01/10/2021 04:21:35 Task [ 0] updates[284000] train loss[0.36348] remaining[0:24:41]
01/10/2021 04:22:44 Task [ 0] updates[284500] train loss[0.36316] remaining[0:23:31]
01/10/2021 04:23:55 Task [ 0] updates[285000] train loss[0.36286] remaining[0:22:20]
01/10/2021 04:25:05 Task [ 0] updates[285500] train loss[0.36249] remaining[0:21:10]
01/10/2021 04:26:16 Task [ 0] updates[286000] train loss[0.36221] remaining[0:20:00]
01/10/2021 04:27:26 Task [ 0] updates[286500] train loss[0.36190] remaining[0:18:49]
01/10/2021 04:28:35 Task [ 0] updates[287000] train loss[0.36156] remaining[0:17:39]
01/10/2021 04:29:45 Task [ 0] updates[287500] train loss[0.36126] remaining[0:16:28]
01/10/2021 04:30:55 Task [ 0] updates[288000] train loss[0.36092] remaining[0:15:18]
