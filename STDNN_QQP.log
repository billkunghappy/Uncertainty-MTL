01/04/2021 07:14:33 1
01/04/2021 07:14:33 Launching the MT-DNN training
01/04/2021 07:14:33 Loading data/canonical_data/bert_base_uncased_lower/qqp_train.json as task 0
01/04/2021 07:14:47 ####################
01/04/2021 07:14:47 {'log_file': 'checkpoints/2021-01-04T1914_bert-base-uncased_qqp/log.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'bert-base-uncased', 'data_dir': 'data/canonical_data/bert_base_uncased_lower', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'experiments/glue/glue_task_def.yml', 'train_datasets': ['qqp'], 'test_datasets': ['qqp'], 'glue_format_on': False, 'mkd_opt': 0, 'do_padding': False, 'update_bert_opt': 0, 'multi_gpu_on': True, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'max_answer_len': 10, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': 1, 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'num_hidden_layers': -1, 'bert_model_type': 'bert-base-uncased', 'do_lower_case': False, 'masked_lm_prob': 0.15, 'short_seq_prob': 0.2, 'max_predictions_per_seq': 128, 'bin_on': False, 'bin_size': 64, 'bin_grow_ratio': 0.5, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 20, 'batch_size': 16, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0.0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'lr_gamma': 0.5, 'scheduler_type': 'ms', 'output_dir': 'checkpoints/2021-01-04T1914_bert-base-uncased_qqp', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'adv_train': False, 'adv_opt': 0, 'adv_norm_level': 0, 'adv_p_norm': 'inf', 'adv_alpha': 1, 'adv_k': 1, 'adv_step_size': 0.001, 'adv_noise_var': 1e-05, 'adv_epsilon': 1e-06, 'loss_pred': True, 'collect_uncertainty': None, 'collect_topk': 0.1, 'load_ranked_data': None, 'mc_dropout': 0, 'finetune': False, 'encode_mode': False, 'task_def_list': [{'self': '{}', 'label_vocab': 'None', 'n_class': '2', 'data_type': '<DataFormat.PremiseAndOneHypothesis: 2>', 'task_type': '<TaskType.Classification: 1>', 'metric_meta': '(<Metric.ACC: 0>, <Metric.F1: 1>)', 'split_names': "['train', 'dev', 'test']", 'enable_san': 'False', 'dropout_p': 'None', 'loss': '<LossCriterion.CeCriterion: 0>', 'kd_loss': '<LossCriterion.MseCriterion: 1>', 'adv_loss': '<LossCriterion.SymKlCriterion: 7>', '__class__': "<class 'experiments.exp_def.TaskDef'>"}]}
01/04/2021 07:14:47 ####################
01/04/2021 07:14:47 ############# Gradient Accumulation Info #############
01/04/2021 07:14:47 number of step: 454820
01/04/2021 07:14:47 number of grad grad_accumulation step: 1
01/04/2021 07:14:47 adjusted number of step: 454820
01/04/2021 07:14:47 ############# Gradient Accumulation Info #############
01/04/2021 07:14:58 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (loss_pred_fc): Linear(in_features=768, out_features=1, bias=True)
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=2, bias=True)
  )
)

01/04/2021 07:14:58 Total number of params: 109484547
01/04/2021 07:14:58 At epoch 0
01/04/2021 07:14:59 Task [ 0] updates[     1] train loss[1.73149] remaining[6:32:11]
01/04/2021 07:15:57 Task [ 0] updates[   500] train loss[1.74221] remaining[0:43:29]
01/04/2021 07:16:52 Task [ 0] updates[  1000] train loss[1.72046] remaining[0:41:20]
01/04/2021 07:17:49 Task [ 0] updates[  1500] train loss[1.69572] remaining[0:40:14]
01/04/2021 07:18:47 Task [ 0] updates[  2000] train loss[1.65372] remaining[0:39:36]
01/04/2021 07:19:49 Task [ 0] updates[  2500] train loss[1.58199] remaining[0:39:10]
01/04/2021 07:20:45 Task [ 0] updates[  3000] train loss[1.51071] remaining[0:37:59]
01/04/2021 07:21:39 Task [ 0] updates[  3500] train loss[1.44698] remaining[0:36:44]
01/04/2021 07:22:44 Task [ 0] updates[  4000] train loss[1.39788] remaining[0:36:19]
01/04/2021 07:23:38 Task [ 0] updates[  4500] train loss[1.35583] remaining[0:35:07]
01/04/2021 07:24:32 Task [ 0] updates[  5000] train loss[1.31849] remaining[0:33:56]
01/04/2021 07:25:28 Task [ 0] updates[  5500] train loss[1.28669] remaining[0:32:53]
01/04/2021 07:26:22 Task [ 0] updates[  6000] train loss[1.25580] remaining[0:31:48]
01/04/2021 07:27:17 Task [ 0] updates[  6500] train loss[1.23045] remaining[0:30:45]
01/04/2021 07:28:11 Task [ 0] updates[  7000] train loss[1.20808] remaining[0:29:43]
01/04/2021 07:29:06 Task [ 0] updates[  7500] train loss[1.18624] remaining[0:28:42]
01/04/2021 07:30:00 Task [ 0] updates[  8000] train loss[1.16597] remaining[0:27:40]
01/04/2021 07:30:58 Task [ 0] updates[  8500] train loss[1.14645] remaining[0:26:47]
01/04/2021 07:31:57 Task [ 0] updates[  9000] train loss[1.12588] remaining[0:25:55]
01/04/2021 07:33:03 Task [ 0] updates[  9500] train loss[1.10721] remaining[0:25:12]
01/04/2021 07:34:07 Task [ 0] updates[ 10000] train loss[1.08937] remaining[0:24:23]
01/04/2021 07:35:17 Task [ 0] updates[ 10500] train loss[1.07525] remaining[0:23:40]
01/04/2021 07:36:20 Task [ 0] updates[ 11000] train loss[1.06175] remaining[0:22:47]
01/04/2021 07:37:27 Task [ 0] updates[ 11500] train loss[1.04701] remaining[0:21:58]
01/04/2021 07:38:31 Task [ 0] updates[ 12000] train loss[1.03108] remaining[0:21:04]
01/04/2021 07:39:39 Task [ 0] updates[ 12500] train loss[1.01754] remaining[0:20:12]
01/04/2021 07:40:42 Task [ 0] updates[ 13000] train loss[1.00433] remaining[0:19:16]
01/04/2021 07:41:46 Task [ 0] updates[ 13500] train loss[0.99268] remaining[0:18:20]
01/04/2021 07:42:48 Task [ 0] updates[ 14000] train loss[0.98166] remaining[0:17:22]
01/04/2021 07:43:51 Task [ 0] updates[ 14500] train loss[0.97038] remaining[0:16:25]
01/04/2021 07:45:00 Task [ 0] updates[ 15000] train loss[0.95896] remaining[0:15:30]
01/04/2021 07:46:05 Task [ 0] updates[ 15500] train loss[0.94860] remaining[0:14:31]
01/04/2021 07:47:15 Task [ 0] updates[ 16000] train loss[0.93952] remaining[0:13:35]
01/04/2021 07:48:36 Task [ 0] updates[ 16500] train loss[0.93070] remaining[0:12:43]
01/04/2021 07:49:43 Task [ 0] updates[ 17000] train loss[0.92317] remaining[0:11:43]
01/04/2021 07:50:48 Task [ 0] updates[ 17500] train loss[0.91466] remaining[0:10:43]
01/04/2021 07:51:50 Task [ 0] updates[ 18000] train loss[0.90698] remaining[0:09:42]
01/04/2021 07:52:56 Task [ 0] updates[ 18500] train loss[0.89867] remaining[0:08:42]
01/04/2021 07:54:06 Task [ 0] updates[ 19000] train loss[0.89233] remaining[0:07:42]
01/04/2021 07:55:07 Task [ 0] updates[ 19500] train loss[0.88543] remaining[0:06:40]
01/04/2021 07:56:08 Task [ 0] updates[ 20000] train loss[0.87855] remaining[0:05:38]
01/04/2021 07:57:04 Task [ 0] updates[ 20500] train loss[0.87142] remaining[0:04:36]
01/04/2021 07:57:57 Task [ 0] updates[ 21000] train loss[0.86535] remaining[0:03:33]
01/04/2021 07:58:52 Task [ 0] updates[ 21500] train loss[0.85982] remaining[0:02:31]
01/04/2021 07:59:46 Task [ 0] updates[ 22000] train loss[0.85415] remaining[0:01:30]
01/04/2021 08:00:41 Task [ 0] updates[ 22500] train loss[0.84893] remaining[0:00:29]
01/04/2021 08:02:13 Task qqp -- epoch 0 -- Dev ACC: 85.469
01/04/2021 08:02:13 Task qqp -- epoch 0 -- Dev F1: 82.082
01/04/2021 08:12:47 [new test scores saved.]
01/04/2021 08:12:52 At epoch 1
01/04/2021 08:13:21 Task [ 0] updates[ 23000] train loss[0.84334] remaining[0:42:04]
01/04/2021 08:14:15 Task [ 0] updates[ 23500] train loss[0.83802] remaining[0:40:22]
01/04/2021 08:15:13 Task [ 0] updates[ 24000] train loss[0.83329] remaining[0:40:09]
01/04/2021 08:16:15 Task [ 0] updates[ 24500] train loss[0.82821] remaining[0:40:27]
01/04/2021 08:17:14 Task [ 0] updates[ 25000] train loss[0.82373] remaining[0:39:38]
01/04/2021 08:18:12 Task [ 0] updates[ 25500] train loss[0.81941] remaining[0:38:40]
01/04/2021 08:19:07 Task [ 0] updates[ 26000] train loss[0.81506] remaining[0:37:24]
01/04/2021 08:20:01 Task [ 0] updates[ 26500] train loss[0.81107] remaining[0:36:10]
01/04/2021 08:20:56 Task [ 0] updates[ 27000] train loss[0.80671] remaining[0:35:02]
01/04/2021 08:21:51 Task [ 0] updates[ 27500] train loss[0.80246] remaining[0:33:56]
01/04/2021 08:22:44 Task [ 0] updates[ 28000] train loss[0.79882] remaining[0:32:50]
01/04/2021 08:23:37 Task [ 0] updates[ 28500] train loss[0.79444] remaining[0:31:43]
01/04/2021 08:24:31 Task [ 0] updates[ 29000] train loss[0.79010] remaining[0:30:42]
01/04/2021 08:25:25 Task [ 0] updates[ 29500] train loss[0.78630] remaining[0:29:41]
01/04/2021 08:26:18 Task [ 0] updates[ 30000] train loss[0.78260] remaining[0:28:40]
01/04/2021 08:27:14 Task [ 0] updates[ 30500] train loss[0.77953] remaining[0:27:45]
01/04/2021 08:28:08 Task [ 0] updates[ 31000] train loss[0.77584] remaining[0:26:46]
01/04/2021 08:29:02 Task [ 0] updates[ 31500] train loss[0.77233] remaining[0:25:48]
01/04/2021 08:29:56 Task [ 0] updates[ 32000] train loss[0.76919] remaining[0:24:51]
01/04/2021 08:30:52 Task [ 0] updates[ 32500] train loss[0.76623] remaining[0:23:57]
01/04/2021 08:31:48 Task [ 0] updates[ 33000] train loss[0.76298] remaining[0:23:02]
01/04/2021 08:32:47 Task [ 0] updates[ 33500] train loss[0.75998] remaining[0:22:11]
01/04/2021 08:33:51 Task [ 0] updates[ 34000] train loss[0.75663] remaining[0:21:24]
01/04/2021 08:34:56 Task [ 0] updates[ 34500] train loss[0.75331] remaining[0:20:37]
01/04/2021 08:36:03 Task [ 0] updates[ 35000] train loss[0.75032] remaining[0:19:49]
01/04/2021 08:37:10 Task [ 0] updates[ 35500] train loss[0.74745] remaining[0:19:00]
01/04/2021 08:38:16 Task [ 0] updates[ 36000] train loss[0.74469] remaining[0:18:10]
01/04/2021 08:39:23 Task [ 0] updates[ 36500] train loss[0.74180] remaining[0:17:19]
01/04/2021 08:40:24 Task [ 0] updates[ 37000] train loss[0.73879] remaining[0:16:22]
01/04/2021 08:41:30 Task [ 0] updates[ 37500] train loss[0.73586] remaining[0:15:29]
01/04/2021 08:42:37 Task [ 0] updates[ 38000] train loss[0.73272] remaining[0:14:35]
01/04/2021 08:43:32 Task [ 0] updates[ 38500] train loss[0.72992] remaining[0:13:35]
01/04/2021 08:44:26 Task [ 0] updates[ 39000] train loss[0.72763] remaining[0:12:35]
01/04/2021 08:45:19 Task [ 0] updates[ 39500] train loss[0.72527] remaining[0:11:35]
01/04/2021 08:46:13 Task [ 0] updates[ 40000] train loss[0.72290] remaining[0:10:35]
01/04/2021 08:47:06 Task [ 0] updates[ 40500] train loss[0.72064] remaining[0:09:36]
01/04/2021 08:47:59 Task [ 0] updates[ 41000] train loss[0.71815] remaining[0:08:37]
01/04/2021 08:48:52 Task [ 0] updates[ 41500] train loss[0.71565] remaining[0:07:38]
01/04/2021 08:49:46 Task [ 0] updates[ 42000] train loss[0.71352] remaining[0:06:40]
01/04/2021 08:50:40 Task [ 0] updates[ 42500] train loss[0.71141] remaining[0:05:42]
01/04/2021 08:51:33 Task [ 0] updates[ 43000] train loss[0.70884] remaining[0:04:44]
01/04/2021 08:52:26 Task [ 0] updates[ 43500] train loss[0.70642] remaining[0:03:46]
01/04/2021 08:53:19 Task [ 0] updates[ 44000] train loss[0.70414] remaining[0:02:49]
01/04/2021 08:54:12 Task [ 0] updates[ 44500] train loss[0.70194] remaining[0:01:51]
01/04/2021 08:55:06 Task [ 0] updates[ 45000] train loss[0.69981] remaining[0:00:54]
01/04/2021 08:57:03 Task qqp -- epoch 1 -- Dev ACC: 87.984
01/04/2021 08:57:03 Task qqp -- epoch 1 -- Dev F1: 83.619
01/04/2021 09:07:58 [new test scores saved.]
01/04/2021 09:08:02 At epoch 2
01/04/2021 09:08:04 Task [ 0] updates[ 45500] train loss[0.69758] remaining[0:43:05]
01/04/2021 09:08:58 Task [ 0] updates[ 46000] train loss[0.69531] remaining[0:40:12]
01/04/2021 09:09:53 Task [ 0] updates[ 46500] train loss[0.69292] remaining[0:39:25]
01/04/2021 09:10:52 Task [ 0] updates[ 47000] train loss[0.69068] remaining[0:39:47]
01/04/2021 09:11:52 Task [ 0] updates[ 47500] train loss[0.68851] remaining[0:39:25]
01/04/2021 09:12:51 Task [ 0] updates[ 48000] train loss[0.68655] remaining[0:38:42]
01/04/2021 09:13:58 Task [ 0] updates[ 48500] train loss[0.68460] remaining[0:38:49]
01/04/2021 09:15:02 Task [ 0] updates[ 49000] train loss[0.68238] remaining[0:38:17]
01/04/2021 09:15:59 Task [ 0] updates[ 49500] train loss[0.68034] remaining[0:37:03]
01/04/2021 09:16:54 Task [ 0] updates[ 50000] train loss[0.67824] remaining[0:35:48]
01/04/2021 09:17:50 Task [ 0] updates[ 50500] train loss[0.67631] remaining[0:34:38]
01/04/2021 09:18:47 Task [ 0] updates[ 51000] train loss[0.67430] remaining[0:33:32]
01/04/2021 09:19:43 Task [ 0] updates[ 51500] train loss[0.67219] remaining[0:32:28]
01/04/2021 09:20:40 Task [ 0] updates[ 52000] train loss[0.67013] remaining[0:31:28]
01/04/2021 09:21:37 Task [ 0] updates[ 52500] train loss[0.66817] remaining[0:30:26]
01/04/2021 09:22:45 Task [ 0] updates[ 53000] train loss[0.66631] remaining[0:29:49]
01/04/2021 09:23:45 Task [ 0] updates[ 53500] train loss[0.66424] remaining[0:28:52]
01/04/2021 09:24:40 Task [ 0] updates[ 54000] train loss[0.66247] remaining[0:27:46]
01/04/2021 09:25:34 Task [ 0] updates[ 54500] train loss[0.66051] remaining[0:26:41]
01/04/2021 09:26:28 Task [ 0] updates[ 55000] train loss[0.65851] remaining[0:25:37]
01/04/2021 09:27:22 Task [ 0] updates[ 55500] train loss[0.65651] remaining[0:24:33]
01/04/2021 09:28:16 Task [ 0] updates[ 56000] train loss[0.65487] remaining[0:23:30]
01/04/2021 09:29:09 Task [ 0] updates[ 56500] train loss[0.65309] remaining[0:22:28]
01/04/2021 09:30:00 Task [ 0] updates[ 57000] train loss[0.65116] remaining[0:21:24]
01/04/2021 09:30:49 Task [ 0] updates[ 57500] train loss[0.64923] remaining[0:20:19]
01/04/2021 09:31:41 Task [ 0] updates[ 58000] train loss[0.64717] remaining[0:19:19]
01/04/2021 09:32:35 Task [ 0] updates[ 58500] train loss[0.64563] remaining[0:18:20]
01/04/2021 09:33:29 Task [ 0] updates[ 59000] train loss[0.64396] remaining[0:17:22]
01/04/2021 09:34:23 Task [ 0] updates[ 59500] train loss[0.64264] remaining[0:16:24]
01/04/2021 09:35:23 Task [ 0] updates[ 60000] train loss[0.64082] remaining[0:15:29]
01/04/2021 09:36:20 Task [ 0] updates[ 60500] train loss[0.63897] remaining[0:14:33]
01/04/2021 09:37:14 Task [ 0] updates[ 61000] train loss[0.63701] remaining[0:13:35]
01/04/2021 09:38:07 Task [ 0] updates[ 61500] train loss[0.63532] remaining[0:12:37]
01/04/2021 09:39:01 Task [ 0] updates[ 62000] train loss[0.63355] remaining[0:11:40]
01/04/2021 09:39:55 Task [ 0] updates[ 62500] train loss[0.63191] remaining[0:10:43]
01/04/2021 09:40:48 Task [ 0] updates[ 63000] train loss[0.63014] remaining[0:09:46]
01/04/2021 09:41:42 Task [ 0] updates[ 63500] train loss[0.62867] remaining[0:08:49]
01/04/2021 09:42:36 Task [ 0] updates[ 64000] train loss[0.62691] remaining[0:07:52]
01/04/2021 09:43:29 Task [ 0] updates[ 64500] train loss[0.62539] remaining[0:06:56]
01/04/2021 09:44:23 Task [ 0] updates[ 65000] train loss[0.62376] remaining[0:06:00]
01/04/2021 09:45:16 Task [ 0] updates[ 65500] train loss[0.62209] remaining[0:05:03]
01/04/2021 09:46:09 Task [ 0] updates[ 66000] train loss[0.62050] remaining[0:04:07]
01/04/2021 09:47:03 Task [ 0] updates[ 66500] train loss[0.61919] remaining[0:03:11]
01/04/2021 09:47:57 Task [ 0] updates[ 67000] train loss[0.61758] remaining[0:02:16]
01/04/2021 09:48:52 Task [ 0] updates[ 67500] train loss[0.61630] remaining[0:01:20]
01/04/2021 09:49:46 Task [ 0] updates[ 68000] train loss[0.61468] remaining[0:00:24]
01/04/2021 09:51:16 Task qqp -- epoch 2 -- Dev ACC: 89.035
01/04/2021 09:51:16 Task qqp -- epoch 2 -- Dev F1: 85.556
01/04/2021 10:01:53 [new test scores saved.]
01/04/2021 10:01:57 At epoch 3
01/04/2021 10:02:27 Task [ 0] updates[ 68500] train loss[0.61313] remaining[0:40:58]
01/04/2021 10:03:22 Task [ 0] updates[ 69000] train loss[0.61156] remaining[0:40:15]
01/04/2021 10:04:17 Task [ 0] updates[ 69500] train loss[0.61009] remaining[0:39:19]
01/04/2021 10:05:10 Task [ 0] updates[ 70000] train loss[0.60856] remaining[0:38:04]
01/04/2021 10:06:04 Task [ 0] updates[ 70500] train loss[0.60704] remaining[0:36:57]
01/04/2021 10:06:58 Task [ 0] updates[ 71000] train loss[0.60558] remaining[0:36:05]
01/04/2021 10:07:53 Task [ 0] updates[ 71500] train loss[0.60411] remaining[0:35:14]
01/04/2021 10:08:47 Task [ 0] updates[ 72000] train loss[0.60269] remaining[0:34:21]
01/04/2021 10:09:42 Task [ 0] updates[ 72500] train loss[0.60116] remaining[0:33:29]
01/04/2021 10:10:38 Task [ 0] updates[ 73000] train loss[0.59971] remaining[0:32:40]
01/04/2021 10:11:32 Task [ 0] updates[ 73500] train loss[0.59842] remaining[0:31:43]
01/04/2021 10:12:25 Task [ 0] updates[ 74000] train loss[0.59687] remaining[0:30:44]
01/04/2021 10:13:19 Task [ 0] updates[ 74500] train loss[0.59540] remaining[0:29:48]
01/04/2021 10:14:13 Task [ 0] updates[ 75000] train loss[0.59410] remaining[0:28:53]
01/04/2021 10:15:07 Task [ 0] updates[ 75500] train loss[0.59329] remaining[0:27:59]
01/04/2021 10:16:01 Task [ 0] updates[ 76000] train loss[0.59207] remaining[0:27:04]
01/04/2021 10:16:56 Task [ 0] updates[ 76500] train loss[0.59057] remaining[0:26:11]
01/04/2021 10:17:50 Task [ 0] updates[ 77000] train loss[0.58935] remaining[0:25:16]
01/04/2021 10:18:44 Task [ 0] updates[ 77500] train loss[0.58788] remaining[0:24:22]
01/04/2021 10:19:38 Task [ 0] updates[ 78000] train loss[0.58650] remaining[0:23:26]
01/04/2021 10:20:31 Task [ 0] updates[ 78500] train loss[0.58520] remaining[0:22:31]
01/04/2021 10:21:24 Task [ 0] updates[ 79000] train loss[0.58432] remaining[0:21:36]
01/04/2021 10:22:17 Task [ 0] updates[ 79500] train loss[0.58315] remaining[0:20:40]
01/04/2021 10:23:11 Task [ 0] updates[ 80000] train loss[0.58171] remaining[0:19:45]
01/04/2021 10:24:04 Task [ 0] updates[ 80500] train loss[0.58044] remaining[0:18:51]
01/04/2021 10:24:57 Task [ 0] updates[ 81000] train loss[0.57921] remaining[0:17:56]
01/04/2021 10:25:50 Task [ 0] updates[ 81500] train loss[0.57805] remaining[0:17:01]
01/04/2021 10:26:43 Task [ 0] updates[ 82000] train loss[0.57674] remaining[0:16:06]
01/04/2021 10:27:36 Task [ 0] updates[ 82500] train loss[0.57541] remaining[0:15:12]
01/04/2021 10:28:30 Task [ 0] updates[ 83000] train loss[0.57418] remaining[0:14:18]
01/04/2021 10:29:24 Task [ 0] updates[ 83500] train loss[0.57288] remaining[0:13:24]
01/04/2021 10:30:28 Task [ 0] updates[ 84000] train loss[0.57160] remaining[0:12:35]
01/04/2021 10:31:36 Task [ 0] updates[ 84500] train loss[0.57034] remaining[0:11:46]
01/04/2021 10:32:46 Task [ 0] updates[ 85000] train loss[0.56902] remaining[0:10:57]
01/04/2021 10:33:57 Task [ 0] updates[ 85500] train loss[0.56772] remaining[0:10:07]
01/04/2021 10:35:07 Task [ 0] updates[ 86000] train loss[0.56645] remaining[0:09:15]
01/04/2021 10:36:17 Task [ 0] updates[ 86500] train loss[0.56524] remaining[0:08:23]
01/04/2021 10:37:21 Task [ 0] updates[ 87000] train loss[0.56406] remaining[0:07:28]
01/04/2021 10:38:15 Task [ 0] updates[ 87500] train loss[0.56291] remaining[0:06:31]
01/04/2021 10:39:12 Task [ 0] updates[ 88000] train loss[0.56176] remaining[0:05:34]
01/04/2021 10:40:06 Task [ 0] updates[ 88500] train loss[0.56043] remaining[0:04:38]
01/04/2021 10:41:00 Task [ 0] updates[ 89000] train loss[0.55967] remaining[0:03:41]
01/04/2021 10:41:53 Task [ 0] updates[ 89500] train loss[0.55873] remaining[0:02:44]
01/04/2021 10:42:47 Task [ 0] updates[ 90000] train loss[0.55780] remaining[0:01:48]
01/04/2021 10:43:40 Task [ 0] updates[ 90500] train loss[0.55683] remaining[0:00:52]
01/04/2021 10:45:35 Task qqp -- epoch 3 -- Dev ACC: 89.540
01/04/2021 10:45:35 Task qqp -- epoch 3 -- Dev F1: 86.377
01/04/2021 10:56:13 [new test scores saved.]
01/04/2021 10:56:17 At epoch 4
01/04/2021 10:56:20 Task [ 0] updates[ 91000] train loss[0.55569] remaining[0:41:02]
01/04/2021 10:57:14 Task [ 0] updates[ 91500] train loss[0.55454] remaining[0:39:39]
01/04/2021 10:58:08 Task [ 0] updates[ 92000] train loss[0.55347] remaining[0:38:55]
01/04/2021 10:59:03 Task [ 0] updates[ 92500] train loss[0.55232] remaining[0:38:11]
01/04/2021 10:59:57 Task [ 0] updates[ 93000] train loss[0.55116] remaining[0:37:17]
01/04/2021 11:00:52 Task [ 0] updates[ 93500] train loss[0.54999] remaining[0:36:31]
01/04/2021 11:01:49 Task [ 0] updates[ 94000] train loss[0.54889] remaining[0:35:56]
01/04/2021 11:02:48 Task [ 0] updates[ 94500] train loss[0.54772] remaining[0:35:27]
01/04/2021 11:03:44 Task [ 0] updates[ 95000] train loss[0.54667] remaining[0:34:32]
01/04/2021 11:04:39 Task [ 0] updates[ 95500] train loss[0.54565] remaining[0:33:35]
01/04/2021 11:05:35 Task [ 0] updates[ 96000] train loss[0.54458] remaining[0:32:44]
01/04/2021 11:06:31 Task [ 0] updates[ 96500] train loss[0.54358] remaining[0:31:50]
01/04/2021 11:07:25 Task [ 0] updates[ 97000] train loss[0.54249] remaining[0:30:51]
01/04/2021 11:08:20 Task [ 0] updates[ 97500] train loss[0.54149] remaining[0:29:52]
01/04/2021 11:09:14 Task [ 0] updates[ 98000] train loss[0.54054] remaining[0:28:55]
01/04/2021 11:10:08 Task [ 0] updates[ 98500] train loss[0.53956] remaining[0:27:58]
01/04/2021 11:11:02 Task [ 0] updates[ 99000] train loss[0.53839] remaining[0:27:00]
01/04/2021 11:11:56 Task [ 0] updates[ 99500] train loss[0.53737] remaining[0:26:04]
01/04/2021 11:12:50 Task [ 0] updates[100000] train loss[0.53637] remaining[0:25:07]
01/04/2021 11:13:45 Task [ 0] updates[100500] train loss[0.53531] remaining[0:24:11]
01/04/2021 11:14:38 Task [ 0] updates[101000] train loss[0.53420] remaining[0:23:14]
01/04/2021 11:15:32 Task [ 0] updates[101500] train loss[0.53330] remaining[0:22:17]
01/04/2021 11:16:24 Task [ 0] updates[102000] train loss[0.53247] remaining[0:21:21]
01/04/2021 11:17:16 Task [ 0] updates[102500] train loss[0.53133] remaining[0:20:23]
01/04/2021 11:18:04 Task [ 0] updates[103000] train loss[0.53032] remaining[0:19:22]
01/04/2021 11:18:52 Task [ 0] updates[103500] train loss[0.52925] remaining[0:18:23]
01/04/2021 11:19:40 Task [ 0] updates[104000] train loss[0.52850] remaining[0:17:25]
01/04/2021 11:20:28 Task [ 0] updates[104500] train loss[0.52771] remaining[0:16:27]
01/04/2021 11:21:17 Task [ 0] updates[105000] train loss[0.52677] remaining[0:15:30]
01/04/2021 11:22:05 Task [ 0] updates[105500] train loss[0.52582] remaining[0:14:33]
01/04/2021 11:22:53 Task [ 0] updates[106000] train loss[0.52497] remaining[0:13:38]
01/04/2021 11:23:41 Task [ 0] updates[106500] train loss[0.52390] remaining[0:12:42]
01/04/2021 11:24:29 Task [ 0] updates[107000] train loss[0.52289] remaining[0:11:47]
01/04/2021 11:25:17 Task [ 0] updates[107500] train loss[0.52196] remaining[0:10:53]
01/04/2021 11:26:05 Task [ 0] updates[108000] train loss[0.52095] remaining[0:09:58]
01/04/2021 11:26:53 Task [ 0] updates[108500] train loss[0.51990] remaining[0:09:05]
01/04/2021 11:27:41 Task [ 0] updates[109000] train loss[0.51896] remaining[0:08:11]
01/04/2021 11:28:29 Task [ 0] updates[109500] train loss[0.51804] remaining[0:07:18]
01/04/2021 11:29:17 Task [ 0] updates[110000] train loss[0.51719] remaining[0:06:25]
01/04/2021 11:30:06 Task [ 0] updates[110500] train loss[0.51624] remaining[0:05:32]
01/04/2021 11:30:54 Task [ 0] updates[111000] train loss[0.51526] remaining[0:04:40]
01/04/2021 11:31:42 Task [ 0] updates[111500] train loss[0.51427] remaining[0:03:48]
01/04/2021 11:32:30 Task [ 0] updates[112000] train loss[0.51339] remaining[0:02:56]
01/04/2021 11:33:18 Task [ 0] updates[112500] train loss[0.51265] remaining[0:02:04]
01/04/2021 11:34:06 Task [ 0] updates[113000] train loss[0.51189] remaining[0:01:12]
01/04/2021 11:34:54 Task [ 0] updates[113500] train loss[0.51097] remaining[0:00:21]
01/04/2021 11:36:19 Task qqp -- epoch 4 -- Dev ACC: 89.975
01/04/2021 11:36:19 Task qqp -- epoch 4 -- Dev F1: 86.805
01/04/2021 11:46:54 [new test scores saved.]
01/04/2021 11:46:58 At epoch 5
01/04/2021 11:47:30 Task [ 0] updates[114000] train loss[0.51020] remaining[0:39:54]
01/04/2021 11:48:25 Task [ 0] updates[114500] train loss[0.50939] remaining[0:39:45]
01/04/2021 11:49:25 Task [ 0] updates[115000] train loss[0.50843] remaining[0:40:39]
01/04/2021 11:50:20 Task [ 0] updates[115500] train loss[0.50752] remaining[0:39:13]
01/04/2021 11:51:14 Task [ 0] updates[116000] train loss[0.50672] remaining[0:38:00]
01/04/2021 11:52:08 Task [ 0] updates[116500] train loss[0.50580] remaining[0:36:51]
01/04/2021 11:53:06 Task [ 0] updates[117000] train loss[0.50494] remaining[0:36:11]
01/04/2021 11:54:12 Task [ 0] updates[117500] train loss[0.50413] remaining[0:36:05]
01/04/2021 11:55:18 Task [ 0] updates[118000] train loss[0.50324] remaining[0:35:48]
01/04/2021 11:56:31 Task [ 0] updates[118500] train loss[0.50244] remaining[0:35:43]
01/04/2021 11:57:39 Task [ 0] updates[119000] train loss[0.50186] remaining[0:35:11]
01/04/2021 11:58:35 Task [ 0] updates[119500] train loss[0.50097] remaining[0:33:58]
01/04/2021 11:59:32 Task [ 0] updates[120000] train loss[0.50009] remaining[0:32:48]
01/05/2021 12:00:29 Task [ 0] updates[120500] train loss[0.49919] remaining[0:31:43]
01/05/2021 12:01:38 Task [ 0] updates[121000] train loss[0.49867] remaining[0:31:02]
01/05/2021 12:02:34 Task [ 0] updates[121500] train loss[0.49778] remaining[0:29:54]
01/05/2021 12:03:29 Task [ 0] updates[122000] train loss[0.49684] remaining[0:28:45]
01/05/2021 12:04:25 Task [ 0] updates[122500] train loss[0.49601] remaining[0:27:39]
01/05/2021 12:05:21 Task [ 0] updates[123000] train loss[0.49518] remaining[0:26:35]
01/05/2021 12:06:18 Task [ 0] updates[123500] train loss[0.49435] remaining[0:25:32]
01/05/2021 12:07:14 Task [ 0] updates[124000] train loss[0.49364] remaining[0:24:29]
01/05/2021 12:08:12 Task [ 0] updates[124500] train loss[0.49303] remaining[0:23:29]
01/05/2021 12:09:15 Task [ 0] updates[125000] train loss[0.49225] remaining[0:22:34]
01/05/2021 12:10:24 Task [ 0] updates[125500] train loss[0.49153] remaining[0:21:44]
01/05/2021 12:11:30 Task [ 0] updates[126000] train loss[0.49073] remaining[0:20:50]
01/05/2021 12:12:46 Task [ 0] updates[126500] train loss[0.49015] remaining[0:20:03]
01/05/2021 12:14:07 Task [ 0] updates[127000] train loss[0.48942] remaining[0:19:17]
01/05/2021 12:15:27 Task [ 0] updates[127500] train loss[0.48864] remaining[0:18:28]
01/05/2021 12:16:44 Task [ 0] updates[128000] train loss[0.48779] remaining[0:17:34]
01/05/2021 12:17:56 Task [ 0] updates[128500] train loss[0.48694] remaining[0:16:37]
01/05/2021 12:19:13 Task [ 0] updates[129000] train loss[0.48619] remaining[0:15:41]
01/05/2021 12:20:34 Task [ 0] updates[129500] train loss[0.48538] remaining[0:14:46]
01/05/2021 12:21:55 Task [ 0] updates[130000] train loss[0.48457] remaining[0:13:49]
01/05/2021 12:23:10 Task [ 0] updates[130500] train loss[0.48372] remaining[0:12:48]
01/05/2021 12:24:27 Task [ 0] updates[131000] train loss[0.48292] remaining[0:11:48]
01/05/2021 12:25:44 Task [ 0] updates[131500] train loss[0.48210] remaining[0:10:46]
01/05/2021 12:27:04 Task [ 0] updates[132000] train loss[0.48134] remaining[0:09:44]
01/05/2021 12:28:24 Task [ 0] updates[132500] train loss[0.48064] remaining[0:08:41]
01/05/2021 12:29:40 Task [ 0] updates[133000] train loss[0.47992] remaining[0:07:37]
01/05/2021 12:30:55 Task [ 0] updates[133500] train loss[0.47916] remaining[0:06:32]
01/05/2021 12:32:14 Task [ 0] updates[134000] train loss[0.47831] remaining[0:05:27]
01/05/2021 12:33:35 Task [ 0] updates[134500] train loss[0.47758] remaining[0:04:21]
01/05/2021 12:34:57 Task [ 0] updates[135000] train loss[0.47683] remaining[0:03:15]
01/05/2021 12:36:13 Task [ 0] updates[135500] train loss[0.47619] remaining[0:02:08]
01/05/2021 12:37:25 Task [ 0] updates[136000] train loss[0.47550] remaining[0:01:00]
01/05/2021 12:40:07 Task qqp -- epoch 5 -- Dev ACC: 90.247
01/05/2021 12:40:07 Task qqp -- epoch 5 -- Dev F1: 87.249
01/05/2021 12:54:26 [new test scores saved.]
01/05/2021 12:54:30 At epoch 6
01/05/2021 12:54:40 Task [ 0] updates[136500] train loss[0.47475] remaining[1:11:01]
01/05/2021 12:55:52 Task [ 0] updates[137000] train loss[0.47402] remaining[0:54:22]
01/05/2021 12:57:12 Task [ 0] updates[137500] train loss[0.47336] remaining[0:55:29]
01/05/2021 12:58:31 Task [ 0] updates[138000] train loss[0.47254] remaining[0:54:41]
01/05/2021 12:59:50 Task [ 0] updates[138500] train loss[0.47181] remaining[0:53:40]
01/05/2021 01:01:03 Task [ 0] updates[139000] train loss[0.47113] remaining[0:51:43]
01/05/2021 01:02:09 Task [ 0] updates[139500] train loss[0.47044] remaining[0:49:18]
01/05/2021 01:03:05 Task [ 0] updates[140000] train loss[0.46972] remaining[0:46:20]
01/05/2021 01:03:59 Task [ 0] updates[140500] train loss[0.46908] remaining[0:43:41]
01/05/2021 01:04:52 Task [ 0] updates[141000] train loss[0.46833] remaining[0:41:21]
01/05/2021 01:05:45 Task [ 0] updates[141500] train loss[0.46761] remaining[0:39:21]
01/05/2021 01:06:40 Task [ 0] updates[142000] train loss[0.46689] remaining[0:37:37]
01/05/2021 01:07:36 Task [ 0] updates[142500] train loss[0.46615] remaining[0:36:04]
01/05/2021 01:08:31 Task [ 0] updates[143000] train loss[0.46544] remaining[0:34:35]
01/05/2021 01:09:26 Task [ 0] updates[143500] train loss[0.46505] remaining[0:33:12]
01/05/2021 01:10:23 Task [ 0] updates[144000] train loss[0.46455] remaining[0:31:55]
01/05/2021 01:11:18 Task [ 0] updates[144500] train loss[0.46407] remaining[0:30:37]
01/05/2021 01:12:11 Task [ 0] updates[145000] train loss[0.46344] remaining[0:29:19]
01/05/2021 01:13:05 Task [ 0] updates[145500] train loss[0.46283] remaining[0:28:04]
01/05/2021 01:13:58 Task [ 0] updates[146000] train loss[0.46219] remaining[0:26:51]
01/05/2021 01:15:03 Task [ 0] updates[146500] train loss[0.46153] remaining[0:25:55]
01/05/2021 01:16:10 Task [ 0] updates[147000] train loss[0.46090] remaining[0:25:00]
01/05/2021 01:17:04 Task [ 0] updates[147500] train loss[0.46030] remaining[0:23:50]
01/05/2021 01:17:57 Task [ 0] updates[148000] train loss[0.45964] remaining[0:22:41]
01/05/2021 01:18:50 Task [ 0] updates[148500] train loss[0.45900] remaining[0:21:34]
01/05/2021 01:19:43 Task [ 0] updates[149000] train loss[0.45838] remaining[0:20:27]
01/05/2021 01:20:36 Task [ 0] updates[149500] train loss[0.45769] remaining[0:19:22]
01/05/2021 01:21:29 Task [ 0] updates[150000] train loss[0.45702] remaining[0:18:17]
01/05/2021 01:22:22 Task [ 0] updates[150500] train loss[0.45639] remaining[0:17:13]
01/05/2021 01:23:15 Task [ 0] updates[151000] train loss[0.45568] remaining[0:16:10]
01/05/2021 01:24:08 Task [ 0] updates[151500] train loss[0.45504] remaining[0:15:07]
01/05/2021 01:25:01 Task [ 0] updates[152000] train loss[0.45433] remaining[0:14:05]
01/05/2021 01:25:54 Task [ 0] updates[152500] train loss[0.45365] remaining[0:13:04]
01/05/2021 01:27:01 Task [ 0] updates[153000] train loss[0.45308] remaining[0:12:09]
01/05/2021 01:28:07 Task [ 0] updates[153500] train loss[0.45255] remaining[0:11:12]
01/05/2021 01:29:12 Task [ 0] updates[154000] train loss[0.45192] remaining[0:10:15]
01/05/2021 01:30:13 Task [ 0] updates[154500] train loss[0.45126] remaining[0:09:16]
01/05/2021 01:31:06 Task [ 0] updates[155000] train loss[0.45091] remaining[0:08:15]
01/05/2021 01:32:00 Task [ 0] updates[155500] train loss[0.45031] remaining[0:07:15]
01/05/2021 01:32:53 Task [ 0] updates[156000] train loss[0.44973] remaining[0:06:15]
01/05/2021 01:33:46 Task [ 0] updates[156500] train loss[0.44902] remaining[0:05:15]
01/05/2021 01:34:39 Task [ 0] updates[157000] train loss[0.44831] remaining[0:04:16]
01/05/2021 01:35:33 Task [ 0] updates[157500] train loss[0.44770] remaining[0:03:17]
01/05/2021 01:36:27 Task [ 0] updates[158000] train loss[0.44707] remaining[0:02:18]
01/05/2021 01:37:20 Task [ 0] updates[158500] train loss[0.44645] remaining[0:01:20]
01/05/2021 01:38:14 Task [ 0] updates[159000] train loss[0.44585] remaining[0:00:21]
01/05/2021 01:39:39 Task qqp -- epoch 6 -- Dev ACC: 90.319
01/05/2021 01:39:39 Task qqp -- epoch 6 -- Dev F1: 87.412
01/05/2021 01:50:27 [new test scores saved.]
01/05/2021 01:50:31 At epoch 7
01/05/2021 01:51:12 Task [ 0] updates[159500] train loss[0.44522] remaining[0:49:32]
01/05/2021 01:52:07 Task [ 0] updates[160000] train loss[0.44470] remaining[0:43:07]
01/05/2021 01:53:08 Task [ 0] updates[160500] train loss[0.44408] remaining[0:42:53]
01/05/2021 01:54:15 Task [ 0] updates[161000] train loss[0.44346] remaining[0:43:12]
01/05/2021 01:55:22 Task [ 0] updates[161500] train loss[0.44289] remaining[0:42:52]
01/05/2021 01:56:20 Task [ 0] updates[162000] train loss[0.44243] remaining[0:41:12]
01/05/2021 01:57:13 Task [ 0] updates[162500] train loss[0.44190] remaining[0:39:18]
01/05/2021 01:58:06 Task [ 0] updates[163000] train loss[0.44151] remaining[0:37:40]
01/05/2021 01:59:00 Task [ 0] updates[163500] train loss[0.44095] remaining[0:36:14]
01/05/2021 01:59:53 Task [ 0] updates[164000] train loss[0.44036] remaining[0:34:54]
01/05/2021 02:00:46 Task [ 0] updates[164500] train loss[0.43974] remaining[0:33:37]
01/05/2021 02:01:39 Task [ 0] updates[165000] train loss[0.43911] remaining[0:32:27]
01/05/2021 02:02:33 Task [ 0] updates[165500] train loss[0.43850] remaining[0:31:20]
01/05/2021 02:03:37 Task [ 0] updates[166000] train loss[0.43800] remaining[0:30:38]
01/05/2021 02:04:43 Task [ 0] updates[166500] train loss[0.43746] remaining[0:29:58]
01/05/2021 02:05:45 Task [ 0] updates[167000] train loss[0.43686] remaining[0:29:06]
01/05/2021 02:06:41 Task [ 0] updates[167500] train loss[0.43621] remaining[0:28:03]
01/05/2021 02:07:34 Task [ 0] updates[168000] train loss[0.43565] remaining[0:26:56]
01/05/2021 02:08:28 Task [ 0] updates[168500] train loss[0.43505] remaining[0:25:53]
01/05/2021 02:09:24 Task [ 0] updates[169000] train loss[0.43446] remaining[0:24:53]
01/05/2021 02:10:18 Task [ 0] updates[169500] train loss[0.43410] remaining[0:23:51]
01/05/2021 02:11:12 Task [ 0] updates[170000] train loss[0.43361] remaining[0:22:49]
01/05/2021 02:12:15 Task [ 0] updates[170500] train loss[0.43297] remaining[0:21:57]
01/05/2021 02:13:21 Task [ 0] updates[171000] train loss[0.43234] remaining[0:21:07]
01/05/2021 02:14:27 Task [ 0] updates[171500] train loss[0.43174] remaining[0:20:16]
01/05/2021 02:15:27 Task [ 0] updates[172000] train loss[0.43122] remaining[0:19:19]
01/05/2021 02:16:20 Task [ 0] updates[172500] train loss[0.43068] remaining[0:18:17]
01/05/2021 02:17:13 Task [ 0] updates[173000] train loss[0.43009] remaining[0:17:15]
01/05/2021 02:18:06 Task [ 0] updates[173500] train loss[0.42950] remaining[0:16:14]
01/05/2021 02:18:59 Task [ 0] updates[174000] train loss[0.42890] remaining[0:15:14]
01/05/2021 02:19:53 Task [ 0] updates[174500] train loss[0.42837] remaining[0:14:14]
01/05/2021 02:20:46 Task [ 0] updates[175000] train loss[0.42785] remaining[0:13:15]
01/05/2021 02:21:41 Task [ 0] updates[175500] train loss[0.42743] remaining[0:12:16]
01/05/2021 02:22:35 Task [ 0] updates[176000] train loss[0.42699] remaining[0:11:18]
01/05/2021 02:23:29 Task [ 0] updates[176500] train loss[0.42641] remaining[0:10:20]
01/05/2021 02:24:22 Task [ 0] updates[177000] train loss[0.42590] remaining[0:09:22]
01/05/2021 02:25:16 Task [ 0] updates[177500] train loss[0.42546] remaining[0:08:24]
01/05/2021 02:26:19 Task [ 0] updates[178000] train loss[0.42487] remaining[0:07:28]
01/05/2021 02:27:16 Task [ 0] updates[178500] train loss[0.42431] remaining[0:06:31]
01/05/2021 02:28:09 Task [ 0] updates[179000] train loss[0.42376] remaining[0:05:33]
01/05/2021 02:29:01 Task [ 0] updates[179500] train loss[0.42324] remaining[0:04:36]
01/05/2021 02:29:54 Task [ 0] updates[180000] train loss[0.42272] remaining[0:03:38]
01/05/2021 02:30:47 Task [ 0] updates[180500] train loss[0.42224] remaining[0:02:41]
01/05/2021 02:31:40 Task [ 0] updates[181000] train loss[0.42189] remaining[0:01:45]
01/05/2021 02:32:34 Task [ 0] updates[181500] train loss[0.42150] remaining[0:00:48]
01/05/2021 02:34:25 Task qqp -- epoch 7 -- Dev ACC: 90.213
01/05/2021 02:34:25 Task qqp -- epoch 7 -- Dev F1: 87.225
01/05/2021 02:45:04 [new test scores saved.]
01/05/2021 02:45:08 At epoch 8
01/05/2021 02:45:16 Task [ 0] updates[182000] train loss[0.42105] remaining[0:40:44]
01/05/2021 02:46:09 Task [ 0] updates[182500] train loss[0.42073] remaining[0:39:28]
01/05/2021 02:47:06 Task [ 0] updates[183000] train loss[0.42023] remaining[0:39:37]
01/05/2021 02:48:03 Task [ 0] updates[183500] train loss[0.41974] remaining[0:39:14]
01/05/2021 02:48:59 Task [ 0] updates[184000] train loss[0.41934] remaining[0:38:28]
01/05/2021 02:49:53 Task [ 0] updates[184500] train loss[0.41883] remaining[0:37:17]
01/05/2021 02:50:48 Task [ 0] updates[185000] train loss[0.41833] remaining[0:36:14]
01/05/2021 02:51:42 Task [ 0] updates[185500] train loss[0.41786] remaining[0:35:12]
01/05/2021 02:52:35 Task [ 0] updates[186000] train loss[0.41738] remaining[0:34:11]
01/05/2021 02:53:31 Task [ 0] updates[186500] train loss[0.41687] remaining[0:33:18]
01/05/2021 02:54:25 Task [ 0] updates[187000] train loss[0.41632] remaining[0:32:20]
01/05/2021 02:55:18 Task [ 0] updates[187500] train loss[0.41580] remaining[0:31:20]
01/05/2021 02:56:11 Task [ 0] updates[188000] train loss[0.41531] remaining[0:30:21]
01/05/2021 02:57:05 Task [ 0] updates[188500] train loss[0.41478] remaining[0:29:22]
01/05/2021 02:57:58 Task [ 0] updates[189000] train loss[0.41433] remaining[0:28:25]
01/05/2021 02:58:51 Task [ 0] updates[189500] train loss[0.41380] remaining[0:27:28]
01/05/2021 02:59:44 Task [ 0] updates[190000] train loss[0.41350] remaining[0:26:31]
01/05/2021 03:00:37 Task [ 0] updates[190500] train loss[0.41313] remaining[0:25:35]
01/05/2021 03:01:30 Task [ 0] updates[191000] train loss[0.41271] remaining[0:24:40]
01/05/2021 03:02:23 Task [ 0] updates[191500] train loss[0.41222] remaining[0:23:44]
01/05/2021 03:03:17 Task [ 0] updates[192000] train loss[0.41168] remaining[0:22:49]
01/05/2021 03:04:10 Task [ 0] updates[192500] train loss[0.41124] remaining[0:21:54]
01/05/2021 03:05:02 Task [ 0] updates[193000] train loss[0.41082] remaining[0:20:58]
01/05/2021 03:05:56 Task [ 0] updates[193500] train loss[0.41029] remaining[0:20:04]
01/05/2021 03:06:49 Task [ 0] updates[194000] train loss[0.40981] remaining[0:19:09]
01/05/2021 03:07:56 Task [ 0] updates[194500] train loss[0.40933] remaining[0:18:26]
01/05/2021 03:09:20 Task [ 0] updates[195000] train loss[0.40894] remaining[0:17:54]
01/05/2021 03:10:43 Task [ 0] updates[195500] train loss[0.40844] remaining[0:17:16]
01/05/2021 03:12:02 Task [ 0] updates[196000] train loss[0.40793] remaining[0:16:33]
01/05/2021 03:13:21 Task [ 0] updates[196500] train loss[0.40745] remaining[0:15:49]
01/05/2021 03:14:33 Task [ 0] updates[197000] train loss[0.40698] remaining[0:14:58]
01/05/2021 03:15:49 Task [ 0] updates[197500] train loss[0.40650] remaining[0:14:07]
01/05/2021 03:17:08 Task [ 0] updates[198000] train loss[0.40599] remaining[0:13:16]
01/05/2021 03:18:28 Task [ 0] updates[198500] train loss[0.40554] remaining[0:12:24]
01/05/2021 03:19:46 Task [ 0] updates[199000] train loss[0.40506] remaining[0:11:30]
01/05/2021 03:21:06 Task [ 0] updates[199500] train loss[0.40459] remaining[0:10:34]
01/05/2021 03:22:18 Task [ 0] updates[200000] train loss[0.40413] remaining[0:09:36]
01/05/2021 03:23:36 Task [ 0] updates[200500] train loss[0.40368] remaining[0:08:38]
01/05/2021 03:24:54 Task [ 0] updates[201000] train loss[0.40318] remaining[0:07:39]
01/05/2021 03:26:14 Task [ 0] updates[201500] train loss[0.40276] remaining[0:06:39]
01/05/2021 03:27:35 Task [ 0] updates[202000] train loss[0.40234] remaining[0:05:38]
01/05/2021 03:28:54 Task [ 0] updates[202500] train loss[0.40187] remaining[0:04:36]
01/05/2021 03:30:06 Task [ 0] updates[203000] train loss[0.40145] remaining[0:03:33]
01/05/2021 03:31:27 Task [ 0] updates[203500] train loss[0.40103] remaining[0:02:30]
01/05/2021 03:32:46 Task [ 0] updates[204000] train loss[0.40062] remaining[0:01:26]
01/05/2021 03:34:05 Task [ 0] updates[204500] train loss[0.40021] remaining[0:00:21]
01/05/2021 03:35:59 Task qqp -- epoch 8 -- Dev ACC: 90.742
01/05/2021 03:35:59 Task qqp -- epoch 8 -- Dev F1: 87.625
01/05/2021 03:50:04 [new test scores saved.]
01/05/2021 03:50:08 At epoch 9
01/05/2021 03:51:01 Task [ 0] updates[205000] train loss[0.39977] remaining[0:59:18]
01/05/2021 03:52:17 Task [ 0] updates[205500] train loss[0.39930] remaining[0:56:43]
01/05/2021 03:53:35 Task [ 0] updates[206000] train loss[0.39885] remaining[0:55:22]
01/05/2021 03:54:56 Task [ 0] updates[206500] train loss[0.39842] remaining[0:54:46]
01/05/2021 03:56:14 Task [ 0] updates[207000] train loss[0.39802] remaining[0:53:24]
01/05/2021 03:57:33 Task [ 0] updates[207500] train loss[0.39755] remaining[0:52:10]
01/05/2021 03:58:52 Task [ 0] updates[208000] train loss[0.39711] remaining[0:50:52]
01/05/2021 04:00:03 Task [ 0] updates[208500] train loss[0.39669] remaining[0:48:58]
01/05/2021 04:01:22 Task [ 0] updates[209000] train loss[0.39637] remaining[0:47:42]
01/05/2021 04:02:42 Task [ 0] updates[209500] train loss[0.39603] remaining[0:46:33]
01/05/2021 04:04:00 Task [ 0] updates[210000] train loss[0.39560] remaining[0:45:18]
01/05/2021 04:05:19 Task [ 0] updates[210500] train loss[0.39515] remaining[0:44:01]
01/05/2021 04:06:37 Task [ 0] updates[211000] train loss[0.39480] remaining[0:42:43]
01/05/2021 04:07:48 Task [ 0] updates[211500] train loss[0.39435] remaining[0:41:08]
01/05/2021 04:09:07 Task [ 0] updates[212000] train loss[0.39393] remaining[0:39:54]
01/05/2021 04:10:26 Task [ 0] updates[212500] train loss[0.39348] remaining[0:38:38]
01/05/2021 04:11:43 Task [ 0] updates[213000] train loss[0.39306] remaining[0:37:20]
01/05/2021 04:13:01 Task [ 0] updates[213500] train loss[0.39262] remaining[0:36:02]
01/05/2021 04:14:17 Task [ 0] updates[214000] train loss[0.39217] remaining[0:34:42]
01/05/2021 04:15:31 Task [ 0] updates[214500] train loss[0.39174] remaining[0:33:20]
01/05/2021 04:16:50 Task [ 0] updates[215000] train loss[0.39139] remaining[0:32:04]
01/05/2021 04:18:09 Task [ 0] updates[215500] train loss[0.39107] remaining[0:30:48]
01/05/2021 04:19:27 Task [ 0] updates[216000] train loss[0.39065] remaining[0:29:30]
01/05/2021 04:20:45 Task [ 0] updates[216500] train loss[0.39023] remaining[0:28:13]
01/05/2021 04:21:58 Task [ 0] updates[217000] train loss[0.38982] remaining[0:26:52]
01/05/2021 04:23:14 Task [ 0] updates[217500] train loss[0.38941] remaining[0:25:33]
01/05/2021 04:24:43 Task [ 0] updates[218000] train loss[0.38906] remaining[0:24:24]
01/05/2021 04:26:01 Task [ 0] updates[218500] train loss[0.38864] remaining[0:23:06]
01/05/2021 04:27:19 Task [ 0] updates[219000] train loss[0.38819] remaining[0:21:49]
01/05/2021 04:28:38 Task [ 0] updates[219500] train loss[0.38775] remaining[0:20:32]
01/05/2021 04:29:49 Task [ 0] updates[220000] train loss[0.38747] remaining[0:19:10]
01/05/2021 04:31:08 Task [ 0] updates[220500] train loss[0.38705] remaining[0:17:53]
01/05/2021 04:32:26 Task [ 0] updates[221000] train loss[0.38662] remaining[0:16:36]
01/05/2021 04:33:46 Task [ 0] updates[221500] train loss[0.38620] remaining[0:15:19]
01/05/2021 04:35:05 Task [ 0] updates[222000] train loss[0.38575] remaining[0:14:01]
01/05/2021 04:36:23 Task [ 0] updates[222500] train loss[0.38531] remaining[0:12:43]
01/05/2021 04:37:31 Task [ 0] updates[223000] train loss[0.38491] remaining[0:11:24]
01/05/2021 04:38:49 Task [ 0] updates[223500] train loss[0.38449] remaining[0:10:06]
01/05/2021 04:40:09 Task [ 0] updates[224000] train loss[0.38407] remaining[0:08:49]
01/05/2021 04:41:27 Task [ 0] updates[224500] train loss[0.38363] remaining[0:07:31]
01/05/2021 04:42:46 Task [ 0] updates[225000] train loss[0.38320] remaining[0:06:14]
